{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMERphS2Rv0G"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "iDHcIx26RIrT",
    "outputId": "3331cb65-05e6-4fe2-a936-686c49ccb4de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.merge import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.losses import *\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k069JrWkRnIc"
   },
   "source": [
    "**Declaring all necesarry functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Smoothening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(y, smooth_factor):\n",
    "    '''Convert a matrix of one-hot row-vector labels into smoothed versions.\n",
    "\n",
    "    # Arguments\n",
    "        y: matrix of one-hot row-vector labels to be smoothed\n",
    "        smooth_factor: label smoothing factor (between 0 and 1)\n",
    "\n",
    "    # Returns\n",
    "        A matrix of smoothed labels.\n",
    "    '''\n",
    "    assert len(y.shape) == 2\n",
    "    if 0 <= smooth_factor <= 1:\n",
    "        # label smoothing ref: https://www.robots.ox.ac.uk/~vgg/rg/papers/reinception.pdf\n",
    "        y *= 1 - smooth_factor\n",
    "        y += smooth_factor / y.shape[1]\n",
    "    else:\n",
    "        raise Exception(\n",
    "            'Invalid label smoothing factor: ' + str(smooth_factor))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rCSrZWBjRZBS"
   },
   "outputs": [],
   "source": [
    "#standard scaling: subtract by mean, and divide by standard deviation for every color channel\n",
    "def get_data():\n",
    "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "  \n",
    "  y_train = np_utils.to_categorical(y_train)\n",
    "  y_test = np_utils.to_categorical(y_test)\n",
    "  \n",
    "  X_train_mean = np.mean(x_train, axis=(0,1,2))\n",
    "  X_train_std = np.std(x_train, axis=(0,1,2))\n",
    "  x_train = (x_train - X_train_mean) / X_train_std\n",
    "  x_test = (x_test - X_train_mean) / X_train_std\n",
    "  \n",
    "  #x_train = x_train.astype('float32')\n",
    "  #x_test = x_test.astype('float32')\n",
    "  #x_train /= 255\n",
    "  #x_test /= 255\n",
    "  \n",
    "  smooth_labels(y_train, .2)\n",
    "\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cOYCe0NRlQc"
   },
   "outputs": [],
   "source": [
    "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
    "  fan = np.prod(shape[:-1])\n",
    "  bound = 1 / math.sqrt(fan)\n",
    "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
    "\n",
    "kernel_initializer = init_pytorch\n",
    "\n",
    "#Function used for making basic blocks of Network\n",
    "def Add_BasicLayer(input, num_filter = 12, dropout_rate = 0, add_pooling = False, do_bn = True):\n",
    "    output = Conv2D(int(num_filter), (3,3), use_bias=False, padding='same')(input)    \n",
    "    if add_pooling:\n",
    "      output = MaxPooling2D(pool_size=(2, 2))(output)\n",
    "    \n",
    "    if do_bn:\n",
    "        output = BatchNormalization()(output)\n",
    "    \n",
    "    output = Activation('relu')(output)\n",
    "    \n",
    "    if dropout_rate>0:\n",
    "      output = Dropout(dropout_rate)(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2Oeqa4oS40f"
   },
   "outputs": [],
   "source": [
    "def accuracy(test_x, test_y, model):\n",
    "    result = model.predict(test_x)\n",
    "    predicted_class = np.argmax(result, axis=1)\n",
    "    true_class = np.argmax(test_y, axis=1)\n",
    "    num_correct = np.sum(predicted_class == true_class) \n",
    "    accuracy = float(num_correct)/result.shape[0]\n",
    "    return (accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vmMYhjRS78c"
   },
   "outputs": [],
   "source": [
    "#Make required Augmentation to the data\n",
    "\n",
    "def get_data_augmenter(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    \n",
    "    def eraser(input_img):\n",
    "      \n",
    "        \n",
    "\n",
    "        input_img = np.pad(input_img, ((4, 4), (4, 4), (0,0)), 'reflect')\n",
    "\n",
    "        toCropX = np.random.randint(0,8)\n",
    "        toCropy = np.random.randint(0,8)\n",
    "        input_img = input_img[toCropX : toCropX+32, toCropy: toCropy+32]\n",
    "\n",
    "        \n",
    "        \n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "        \n",
    "        #Doing cutout for every image\n",
    "        #if p_1 > p:\n",
    "         #   return input_img\n",
    "\n",
    "        while True:\n",
    "            w = 5\n",
    "            h = 5\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = 0.5\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iGxVYNAbi3H"
   },
   "source": [
    "## Load data and Create ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "i3JkB_DDUqq6",
    "outputId": "a2fea07f-0f02-4935-9489-50ee385bdee3"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QHkobT_3T3Kx"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range=0.0, \n",
    "                             horizontal_flip=True,\n",
    "                            preprocessing_function=get_data_augmenter(v_l=0, v_h=1, pixel_level=False))\n",
    "\n",
    "#datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6igH-VlKYjgo"
   },
   "source": [
    "#### To display augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40pLeXvAXhZc"
   },
   "outputs": [],
   "source": [
    "iterator1 = datagen.flow(x_train, y_train, batch_size=128, shuffle=False)\n",
    "batch_testX, batch_testy = iterator1.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "6Z9wWCQQU52S",
    "outputId": "f5087f54-1ebf-49b7-c4b0-35cd17e235f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testimg = x_train[4]\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "\n",
    "plt.imshow(batch_testX[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss():\n",
    "    def loss(y_true,y_pred):\n",
    "      raw_weights = model.get_weights()\n",
    "      modified_weights = []\n",
    "      for layer_weight in raw_weights:\n",
    "        modified_weights.append(layer_weight - (WEIGHT_DECAY * BATCH_SIZE * layer_weight))\n",
    "\n",
    "      model.set_weights(modified_weights)\n",
    "\n",
    "      loss = categorical_crossentropy(y_true, y_pred)\n",
    "      return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6GnWdq4b5Lu"
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "SehcU1GFb9dN",
    "outputId": "89b8cba7-2821-422a-9004-06c850805ba9"
   },
   "outputs": [],
   "source": [
    "c=32\n",
    "input = Input(shape=(32, 32, 3,))\n",
    "prep = Add_BasicLayer(input, num_filter = c * 3, dropout_rate = 0, add_pooling = False)\n",
    "prep = SpatialDropout2D(0.15)(prep)\n",
    "layer1_1 = Add_BasicLayer(prep, num_filter = c * 6, dropout_rate = 0, add_pooling = True)\n",
    "layer1_1 = SpatialDropout2D(0.15)(layer1_1)\n",
    "layer1_2 = Add_BasicLayer(layer1_1, num_filter = c * 6, dropout_rate = 0, add_pooling = False)\n",
    "layer1_2 = SpatialDropout2D(0.15)(layer1_2)\n",
    "layer1_3 = Add_BasicLayer(layer1_2, num_filter = c * 6, dropout_rate = 0, add_pooling = False)\n",
    "layer1_3 = SpatialDropout2D(0.15)(layer1_3)\n",
    "#merge1 = concatenate([layer1_1, layer1_3], name='merge1')\n",
    "merge1 = keras.layers.add([layer1_1, layer1_3])\n",
    "\n",
    "#Conv2D_1 = Conv2D(32, (1,1), use_bias=False, padding='same')(merge1)\n",
    "\n",
    "\n",
    "layer2 = Add_BasicLayer(merge1, num_filter = c * 12, dropout_rate = 0, add_pooling = True)\n",
    "\n",
    "layer3_1 = Add_BasicLayer(layer2, num_filter = c * 18, dropout_rate = 0, add_pooling = True, do_bn = False)\n",
    "layer3_1 = SpatialDropout2D(0.15)(layer3_1)\n",
    "layer3_2 = Add_BasicLayer(layer3_1, num_filter = c * 18, dropout_rate = 0, add_pooling = False, do_bn = False)\n",
    "#layer3_2 = SpatialDropout2D(0.15)(layer3_2)\n",
    "layer3_3 = Add_BasicLayer(layer3_2, num_filter = c * 18, dropout_rate = 0, add_pooling = False, do_bn = False)\n",
    "#merge2= concatenate([layer3_1, layer3_3], name='merge2')\n",
    "merge2 = keras.layers.add([layer3_1, layer3_3])\n",
    "\n",
    "# output = Conv2D(10, kernel_size=(1,1))(merge2)\n",
    "# output = AveragePooling2D(pool_size = (4,4))(output)\n",
    "# output = Flatten()(output)\n",
    "\n",
    "output = AveragePooling2D(pool_size = (4,4))(merge2)\n",
    "output = Flatten()(output)\n",
    "output = Dense(10)(output)\n",
    "\n",
    "output = Lambda(lambda y: y * 0.125 )(output)\n",
    "output = Activation('softmax')(output)\n",
    "\n",
    "model = Model(inputs=[input], outputs=[output])\n",
    "\n",
    "#sgd = tf.keras.optimizers.SGD(nesterov=True)\n",
    "\n",
    "sgd = optimizers.SGD(nesterov=True)\n",
    "loss = custom_loss()\n",
    "model.compile(loss=loss,\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Czd6mWgU09SR",
    "outputId": "f85b10ea-049e-438f-95f0-4a29341f7e09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 96)   2592        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 96)   384         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 96)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_1 (SpatialDro (None, 32, 32, 96)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 192)  165888      spatial_dropout2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 192)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 192)  768         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 192)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_2 (SpatialDro (None, 16, 16, 192)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 192)  331776      spatial_dropout2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 192)  768         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 192)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_3 (SpatialDro (None, 16, 16, 192)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 192)  331776      spatial_dropout2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 192)  768         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_4 (SpatialDro (None, 16, 16, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 192)  0           spatial_dropout2d_2[0][0]        \n",
      "                                                                 spatial_dropout2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 384)  663552      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 384)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 384)    1536        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 384)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 576)    1990656     activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 576)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4, 4, 576)    0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_5 (SpatialDro (None, 4, 4, 576)    0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 4, 4, 576)    2985984     spatial_dropout2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 4, 4, 576)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 4, 4, 576)    2985984     activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 4, 4, 576)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 4, 4, 576)    0           spatial_dropout2d_5[0][0]        \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 576)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 576)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           5770        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 10)           0           lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,468,202\n",
      "Trainable params: 9,466,090\n",
      "Non-trainable params: 2,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCJM62tH-Hf8"
   },
   "source": [
    "##One Cyle LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMBRwm0q-GRm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# Code is ported from https://github.com/fastai/fastai\n",
    "class OneCycleLR(Callback):\n",
    "    def __init__(self,\n",
    "                 max_lr,\n",
    "                 end_percentage=0.1,\n",
    "                 scale_percentage=None,\n",
    "                 maximum_momentum=0.95,\n",
    "                 minimum_momentum=0.85,\n",
    "                 verbose=True):\n",
    "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
    "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
    "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
    "        100th its initial lowest value.\n",
    "\n",
    "        # Arguments:\n",
    "            max_lr: Float. Initial learning rate. This also sets the\n",
    "                starting learning rate (which will be 10x smaller than\n",
    "                this), and will increase to this value during the first cycle.\n",
    "            end_percentage: Float. The percentage of all the epochs of training\n",
    "                that will be dedicated to sharply decreasing the learning\n",
    "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
    "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
    "                If None, it will compute the scale_percentage automatically\n",
    "                based on the `end_percentage`.\n",
    "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
    "                value, which gradually drops to its lowest value in half-cycle,\n",
    "                then gradually increases again to stay constant at this max value.\n",
    "                Can only be used with SGD Optimizer.\n",
    "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
    "                the half-cycle. Can only be used with SGD Optimizer.\n",
    "            verbose: Bool. Whether to print the current learning rate after every\n",
    "                epoch.\n",
    "\n",
    "        # Reference\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
    "        \"\"\"\n",
    "        super(OneCycleLR, self).__init__()\n",
    "\n",
    "        if end_percentage < 0. or end_percentage > 1.:\n",
    "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
    "\n",
    "        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n",
    "            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n",
    "\n",
    "        self.initial_lr = max_lr\n",
    "        self.end_percentage = end_percentage\n",
    "        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n",
    "        self.max_momentum = maximum_momentum\n",
    "        self.min_momentum = minimum_momentum\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.max_momentum is not None and self.min_momentum is not None:\n",
    "            self._update_momentum = True\n",
    "        else:\n",
    "            self._update_momentum = False\n",
    "\n",
    "        self.clr_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self.epochs = None\n",
    "        self.batch_size = None\n",
    "        self.samples = None\n",
    "        self.steps = None\n",
    "        self.num_iterations = None\n",
    "        self.mid_cycle_id = None\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the callback.\n",
    "        \"\"\"\n",
    "        self.clr_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "    def compute_lr(self):\n",
    "        \"\"\"\n",
    "        Compute the learning rate based on which phase of the cycle it is in.\n",
    "\n",
    "        - If in the first half of training, the learning rate gradually increases.\n",
    "        - If in the second half of training, the learning rate gradually decreases.\n",
    "        - If in the final `end_percentage` portion of training, the learning rate\n",
    "            is quickly reduced to near 100th of the original min learning rate.\n",
    "\n",
    "        # Returns:\n",
    "            the new learning rate\n",
    "        \"\"\"\n",
    "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
    "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
    "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
    "            new_lr = self.initial_lr * (1. + (current_percentage *\n",
    "                                              (1. - 100.) / 100.)) * self.scale\n",
    "\n",
    "        elif self.clr_iterations > self.mid_cycle_id:\n",
    "            current_percentage = 1. - (\n",
    "                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
    "            new_lr = self.initial_lr * (1. + current_percentage *\n",
    "                                        (self.scale * 100 - 1.)) * self.scale\n",
    "\n",
    "        else:\n",
    "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
    "            new_lr = self.initial_lr * (1. + current_percentage *\n",
    "                                        (self.scale * 100 - 1.)) * self.scale\n",
    "\n",
    "        if self.clr_iterations == self.num_iterations:\n",
    "            self.clr_iterations = 0\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "    def compute_momentum(self):\n",
    "        \"\"\"\n",
    "         Compute the momentum based on which phase of the cycle it is in.\n",
    "\n",
    "        - If in the first half of training, the momentum gradually decreases.\n",
    "        - If in the second half of training, the momentum gradually increases.\n",
    "        - If in the final `end_percentage` portion of training, the momentum value\n",
    "            is kept constant at the maximum initial value.\n",
    "\n",
    "        # Returns:\n",
    "            the new momentum value\n",
    "        \"\"\"\n",
    "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
    "            new_momentum = self.max_momentum\n",
    "\n",
    "        elif self.clr_iterations > self.mid_cycle_id:\n",
    "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
    "                                        self.mid_cycle_id))\n",
    "            new_momentum = self.max_momentum - current_percentage * (\n",
    "                self.max_momentum - self.min_momentum)\n",
    "\n",
    "        else:\n",
    "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
    "            new_momentum = self.max_momentum - current_percentage * (\n",
    "                self.max_momentum - self.min_momentum)\n",
    "\n",
    "        return new_momentum\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        self.epochs = self.params['epochs']\n",
    "        #self.batch_size = self.params['batch_size']\n",
    "        self.batch_size = 512\n",
    "        #self.samples = self.params['samples']\n",
    "        self.samples = 50000\n",
    "        #self.steps = self.params['steps']\n",
    "        self.steps = 98\n",
    "\n",
    "        if self.steps is not None:\n",
    "            self.num_iterations = self.epochs * self.steps\n",
    "        else:\n",
    "            if (self.samples % self.batch_size) == 0:\n",
    "                remainder = 0\n",
    "            else:\n",
    "                remainder = 1\n",
    "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
    "\n",
    "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
    "\n",
    "        self._reset()\n",
    "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
    "\n",
    "        if self._update_momentum:\n",
    "            if not hasattr(self.model.optimizer, 'momentum'):\n",
    "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
    "\n",
    "            new_momentum = self.compute_momentum()\n",
    "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        self.clr_iterations += 1\n",
    "        new_lr = self.compute_lr()\n",
    "\n",
    "        self.history.setdefault('lr', []).append(\n",
    "            K.get_value(self.model.optimizer.lr))\n",
    "        K.set_value(self.model.optimizer.lr, new_lr)\n",
    "\n",
    "        if self._update_momentum:\n",
    "            if not hasattr(self.model.optimizer, 'momentum'):\n",
    "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
    "\n",
    "            new_momentum = self.compute_momentum()\n",
    "\n",
    "            self.history.setdefault('momentum', []).append(\n",
    "                K.get_value(self.model.optimizer.momentum))\n",
    "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.verbose:\n",
    "            if self._update_momentum:\n",
    "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
    "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
    "\n",
    "            else:\n",
    "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
    "\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self,\n",
    "                 num_samples,\n",
    "                 batch_size,\n",
    "                 minimum_lr=1e-5,\n",
    "                 maximum_lr=10.,\n",
    "                 lr_scale='exp',\n",
    "                 validation_data=None,\n",
    "                 validation_sample_rate=5,\n",
    "                 stopping_criterion_factor=4.,\n",
    "                 loss_smoothing_beta=0.98,\n",
    "                 save_dir=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        This class uses the Cyclic Learning Rate history to find a\n",
    "        set of learning rates that can be good initializations for the\n",
    "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
    "        below.\n",
    "\n",
    "        A port of the Fast.ai implementation for Keras.\n",
    "\n",
    "        # Note\n",
    "        This requires that the model be trained for exactly 1 epoch. If the model\n",
    "        is trained for more epochs, then the metric calculations are only done for\n",
    "        the first epoch.\n",
    "\n",
    "        # Interpretation\n",
    "        Upon visualizing the loss plot, check where the loss starts to increase\n",
    "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
    "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
    "        Choose the max value as this value when passing the `max_val` argument\n",
    "        to OneCycleLR callback.\n",
    "\n",
    "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
    "\n",
    "        # Arguments:\n",
    "            num_samples: Integer. Number of samples in the dataset.\n",
    "            batch_size: Integer. Batch size during training.\n",
    "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
    "            maximum_lr: Float. Final learning rate (and the maximum).\n",
    "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
    "                scaling for each update to the learning rate during subsequent\n",
    "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
    "            validation_data: Requires the validation dataset as a tuple of\n",
    "                (X, y) belonging to the validation set. If provided, will use the\n",
    "                validation set to compute the loss metrics. Else uses the training\n",
    "                batch loss. Will warn if not provided to alert the user.\n",
    "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
    "                validation set per iteration of the LRFinder. Larger number of\n",
    "                samples will reduce the variance but will take longer time to execute\n",
    "                per batch.\n",
    "\n",
    "                If Positive > 0, will sample from the validation dataset\n",
    "                If Megative, will use the entire dataset\n",
    "            stopping_criterion_factor: Integer or None. A factor which is used\n",
    "                to measure large increase in the loss value during training.\n",
    "                Since callbacks cannot stop training of a model, it will simply\n",
    "                stop logging the additional values from the epochs after this\n",
    "                stopping criterion has been met.\n",
    "                If None, this check will not be performed.\n",
    "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
    "                average of the loss function.\n",
    "            save_dir: Optional, String. If passed a directory path, the callback\n",
    "                will save the running loss and learning rates to two separate numpy\n",
    "                arrays inside this directory. If the directory in this path does not\n",
    "                exist, they will be created.\n",
    "            verbose: Whether to print the learning rate after every batch of training.\n",
    "\n",
    "        # References:\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "        \"\"\"\n",
    "        super(LRFinder, self).__init__()\n",
    "\n",
    "        if lr_scale not in ['exp', 'linear']:\n",
    "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
    "\n",
    "        if validation_data is not None:\n",
    "            self.validation_data = validation_data\n",
    "            self.use_validation_set = True\n",
    "\n",
    "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
    "                self.validation_sample_rate = validation_sample_rate\n",
    "            else:\n",
    "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n",
    "        else:\n",
    "            self.use_validation_set = False\n",
    "            self.validation_sample_rate = 0\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_lr = minimum_lr\n",
    "        self.final_lr = maximum_lr\n",
    "        self.lr_scale = lr_scale\n",
    "        self.stopping_criterion_factor = stopping_criterion_factor\n",
    "        self.loss_smoothing_beta = loss_smoothing_beta\n",
    "        self.save_dir = save_dir\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.num_batches_ = num_samples // batch_size\n",
    "        self.current_lr_ = minimum_lr\n",
    "\n",
    "        if lr_scale == 'exp':\n",
    "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
    "                1. / float(self.num_batches_))\n",
    "        else:\n",
    "            extra_batch = int((num_samples % batch_size) != 0)\n",
    "            self.lr_multiplier_ = np.linspace(\n",
    "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
    "\n",
    "        # If negative, use entire validation set\n",
    "        if self.validation_sample_rate < 0:\n",
    "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
    "\n",
    "        self.current_batch_ = 0\n",
    "        self.current_epoch_ = 0\n",
    "        self.best_loss_ = 1e6\n",
    "        self.running_loss_ = 0.\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.current_epoch_ = 1\n",
    "        K.set_value(self.model.optimizer.lr, self.initial_lr)\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_batch_ = 0\n",
    "\n",
    "        if self.current_epoch_ > 1:\n",
    "            warnings.warn(\n",
    "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
    "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.current_batch_ += 1\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.current_epoch_ > 1:\n",
    "            return\n",
    "\n",
    "        if self.use_validation_set:\n",
    "            X, Y = self.validation_data[0], self.validation_data[1]\n",
    "\n",
    "            # use 5 random batches from test set for fast approximate of loss\n",
    "            num_samples = self.batch_size * self.validation_sample_rate\n",
    "\n",
    "            if num_samples > X.shape[0]:\n",
    "                num_samples = X.shape[0]\n",
    "\n",
    "            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
    "            x = X[idx]\n",
    "            y = Y[idx]\n",
    "\n",
    "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
    "            loss = values[0]\n",
    "        else:\n",
    "            loss = logs['loss']\n",
    "\n",
    "        # smooth the loss value and bias correct\n",
    "        running_loss = self.loss_smoothing_beta * loss + (\n",
    "            1. - self.loss_smoothing_beta) * loss\n",
    "        running_loss = running_loss / (\n",
    "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
    "\n",
    "        # stop logging if loss is too large\n",
    "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
    "                running_loss >\n",
    "                self.stopping_criterion_factor * self.best_loss_):\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
    "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
    "            return\n",
    "\n",
    "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
    "            self.best_loss_ = running_loss\n",
    "\n",
    "        current_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
    "        if self.lr_scale == 'exp':\n",
    "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
    "        else:\n",
    "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
    "\n",
    "        # compute the lr for the next batch and update the optimizer lr\n",
    "        if self.lr_scale == 'exp':\n",
    "            current_lr *= self.lr_multiplier_\n",
    "        else:\n",
    "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, current_lr)\n",
    "\n",
    "        # save the other metrics as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        if self.verbose:\n",
    "            if self.use_validation_set:\n",
    "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
    "                      (values[0], current_lr))\n",
    "            else:\n",
    "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
    "            if not os.path.exists(self.save_dir):\n",
    "                os.makedirs(self.save_dir)\n",
    "\n",
    "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
    "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
    "\n",
    "            np.save(losses_path, self.losses)\n",
    "            np.save(lrs_path, self.lrs)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
    "                      % (self.save_dir))\n",
    "\n",
    "        self.current_epoch_ += 1\n",
    "\n",
    "        warnings.simplefilter(\"default\")\n",
    "\n",
    "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the callback itself.\n",
    "\n",
    "        # Arguments:\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses = self.losses\n",
    "        lrs = self.lrs\n",
    "\n",
    "        if clip_beginning:\n",
    "            losses = losses[clip_beginning:]\n",
    "            lrs = lrs[clip_beginning:]\n",
    "\n",
    "        if clip_endding:\n",
    "            losses = losses[:clip_endding]\n",
    "            lrs = lrs[:clip_endding]\n",
    "\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.title('Learning rate vs Loss')\n",
    "        plt.xlabel('learning rate')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def restore_schedule_from_dir(cls,\n",
    "                                  directory,\n",
    "                                  clip_beginning=None,\n",
    "                                  clip_endding=None):\n",
    "        \"\"\"\n",
    "        Loads the training history from the saved numpy files in the given directory.\n",
    "\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "\n",
    "        Returns:\n",
    "            tuple of (losses, learning rates)\n",
    "        \"\"\"\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses_path = os.path.join(directory, 'losses.npy')\n",
    "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
    "\n",
    "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
    "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
    "                  (losses_path, lrs_path, directory))\n",
    "\n",
    "            losses = None\n",
    "            lrs = None\n",
    "\n",
    "        else:\n",
    "            losses = np.load(losses_path)\n",
    "            lrs = np.load(lrs_path)\n",
    "\n",
    "            if clip_beginning:\n",
    "                losses = losses[clip_beginning:]\n",
    "                lrs = lrs[clip_beginning:]\n",
    "\n",
    "            if clip_endding:\n",
    "                losses = losses[:clip_endding]\n",
    "                lrs = lrs[:clip_endding]\n",
    "\n",
    "        return losses, lrs\n",
    "\n",
    "    @classmethod\n",
    "    def plot_schedule_from_file(cls,\n",
    "                                directory,\n",
    "                                clip_beginning=None,\n",
    "                                clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
    "        rate values in the specified directory.\n",
    "\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
    "            return\n",
    "\n",
    "        losses, lrs = cls.restore_schedule_from_dir(\n",
    "            directory,\n",
    "            clip_beginning=clip_beginning,\n",
    "            clip_endding=clip_endding)\n",
    "\n",
    "        if losses is None or lrs is None:\n",
    "            return\n",
    "        else:\n",
    "            plt.plot(lrs, losses)\n",
    "            plt.title('Learning rate vs Loss')\n",
    "            plt.xlabel('learning rate')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "\n",
    "    @property\n",
    "    def lrs(self):\n",
    "        return np.array(self.history['log_lrs'])\n",
    "\n",
    "    @property\n",
    "    def losses(self):\n",
    "        return np.array(self.history['running_loss_'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpvpvgNB_xd5"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3n5HSmh4avm"
   },
   "outputs": [],
   "source": [
    "lr_manager = OneCycleLR(\n",
    "    max_lr=2.5,\n",
    "    end_percentage=0.3,\n",
    "    scale_percentage=0.1,\n",
    "    maximum_momentum=0.95,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do validation for only last few epochs\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(epoch > 46):\n",
    "            x, y = self.test_data\n",
    "            loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "            print('\\nTesting loss: {}, Val acc: {}\\n'.format(loss, acc))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GZCenjSa4hiG",
    "outputId": "e0d4c210-785d-441c-ff9b-e7057808b849"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., verbose=1, callbacks=[<__main__..., steps_per_epoch=97, epochs=50)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "97/97 [==============================] - 19s 193ms/step - loss: 1.9125 - acc: 0.3857\n",
      " - lr: 0.37595 - momentum: 0.94 \n",
      "Epoch 2/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.6295 - acc: 0.5750\n",
      " - lr: 0.50321 - momentum: 0.94 \n",
      "Epoch 3/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.4844 - acc: 0.6664\n",
      " - lr: 0.63047 - momentum: 0.93 \n",
      "Epoch 4/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.3750 - acc: 0.7300\n",
      " - lr: 0.75773 - momentum: 0.93 \n",
      "Epoch 5/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 1.3023 - acc: 0.7727\n",
      " - lr: 0.88499 - momentum: 0.92 \n",
      "Epoch 6/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 1.2489 - acc: 0.8024\n",
      " - lr: 1.01224 - momentum: 0.92 \n",
      "Epoch 7/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 1.2094 - acc: 0.8253\n",
      " - lr: 1.13950 - momentum: 0.91 \n",
      "Epoch 8/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.1861 - acc: 0.8383\n",
      " - lr: 1.26676 - momentum: 0.90 \n",
      "Epoch 9/50\n",
      "97/97 [==============================] - 14s 139ms/step - loss: 1.1574 - acc: 0.8551\n",
      " - lr: 1.39402 - momentum: 0.90 \n",
      "Epoch 10/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.1373 - acc: 0.8656\n",
      " - lr: 1.52128 - momentum: 0.89 \n",
      "Epoch 11/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.1221 - acc: 0.8740\n",
      " - lr: 1.64854 - momentum: 0.89 \n",
      "Epoch 12/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.1076 - acc: 0.8837\n",
      " - lr: 1.77580 - momentum: 0.88 \n",
      "Epoch 13/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.0943 - acc: 0.8908\n",
      " - lr: 1.90306 - momentum: 0.88 \n",
      "Epoch 14/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 1.0864 - acc: 0.8961\n",
      " - lr: 2.03032 - momentum: 0.87 \n",
      "Epoch 15/50\n",
      "97/97 [==============================] - 13s 139ms/step - loss: 1.0703 - acc: 0.9036\n",
      " - lr: 2.15758 - momentum: 0.87 \n",
      "Epoch 16/50\n",
      "97/97 [==============================] - 13s 139ms/step - loss: 1.0644 - acc: 0.9087\n",
      " - lr: 2.28484 - momentum: 0.86 \n",
      "Epoch 17/50\n",
      "97/97 [==============================] - 13s 139ms/step - loss: 1.0549 - acc: 0.9140\n",
      " - lr: 2.41210 - momentum: 0.85 \n",
      "Epoch 18/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.0453 - acc: 0.9190\n",
      " - lr: 2.46064 - momentum: 0.85 \n",
      "Epoch 19/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.0313 - acc: 0.9249\n",
      " - lr: 2.33338 - momentum: 0.86 \n",
      "Epoch 20/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.0193 - acc: 0.9310\n",
      " - lr: 2.20612 - momentum: 0.86 \n",
      "Epoch 21/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 1.0108 - acc: 0.9338\n",
      " - lr: 2.07886 - momentum: 0.87 \n",
      "Epoch 22/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 1.0019 - acc: 0.9381\n",
      " - lr: 1.95160 - momentum: 0.87 \n",
      "Epoch 23/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9923 - acc: 0.9440\n",
      " - lr: 1.82434 - momentum: 0.88 \n",
      "Epoch 24/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9824 - acc: 0.9478\n",
      " - lr: 1.69708 - momentum: 0.89 \n",
      "Epoch 25/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9770 - acc: 0.9504\n",
      " - lr: 1.56983 - momentum: 0.89 \n",
      "Epoch 26/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9709 - acc: 0.9536\n",
      " - lr: 1.44257 - momentum: 0.90 \n",
      "Epoch 27/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9652 - acc: 0.9563\n",
      " - lr: 1.31531 - momentum: 0.90 \n",
      "Epoch 28/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9570 - acc: 0.9614\n",
      " - lr: 1.18805 - momentum: 0.91 \n",
      "Epoch 29/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9529 - acc: 0.9630\n",
      " - lr: 1.06079 - momentum: 0.91 \n",
      "Epoch 30/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9476 - acc: 0.9658\n",
      " - lr: 0.93353 - momentum: 0.92 \n",
      "Epoch 31/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9427 - acc: 0.9681\n",
      " - lr: 0.80627 - momentum: 0.93 \n",
      "Epoch 32/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9379 - acc: 0.9704\n",
      " - lr: 0.67901 - momentum: 0.93 \n",
      "Epoch 33/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9337 - acc: 0.9733\n",
      " - lr: 0.55175 - momentum: 0.94 \n",
      "Epoch 34/50\n",
      "97/97 [==============================] - 13s 136ms/step - loss: 0.9280 - acc: 0.9756\n",
      " - lr: 0.42449 - momentum: 0.94 \n",
      "Epoch 35/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9226 - acc: 0.9786\n",
      " - lr: 0.29723 - momentum: 0.95 \n",
      "Epoch 36/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9186 - acc: 0.9807\n",
      " - lr: 0.23973 - momentum: 0.95 \n",
      "Epoch 37/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9140 - acc: 0.9829\n",
      " - lr: 0.22340 - momentum: 0.95 \n",
      "Epoch 38/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9131 - acc: 0.9826\n",
      " - lr: 0.20707 - momentum: 0.95 \n",
      "Epoch 39/50\n",
      "97/97 [==============================] - 13s 136ms/step - loss: 0.9106 - acc: 0.9851\n",
      " - lr: 0.19073 - momentum: 0.95 \n",
      "Epoch 40/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9096 - acc: 0.9854\n",
      " - lr: 0.17440 - momentum: 0.95 \n",
      "Epoch 41/50\n",
      "97/97 [==============================] - 13s 136ms/step - loss: 0.9092 - acc: 0.9854\n",
      " - lr: 0.15807 - momentum: 0.95 \n",
      "Epoch 42/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9084 - acc: 0.9852\n",
      " - lr: 0.14174 - momentum: 0.95 \n",
      "Epoch 43/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9066 - acc: 0.9867\n",
      " - lr: 0.12541 - momentum: 0.95 \n",
      "Epoch 44/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9042 - acc: 0.9882\n",
      " - lr: 0.10908 - momentum: 0.95 \n",
      "Epoch 45/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9032 - acc: 0.9879\n",
      " - lr: 0.09274 - momentum: 0.95 \n",
      "Epoch 46/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9032 - acc: 0.9882\n",
      " - lr: 0.07641 - momentum: 0.95 \n",
      "Epoch 47/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9030 - acc: 0.9890\n",
      " - lr: 0.06008 - momentum: 0.95 \n",
      "Epoch 48/50\n",
      "97/97 [==============================] - 13s 137ms/step - loss: 0.9015 - acc: 0.9891\n",
      " - lr: 0.04375 - momentum: 0.95 \n",
      "\n",
      "Testing loss: 0.3649625330448151, Val acc: 0.9393\n",
      "\n",
      "Epoch 49/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9018 - acc: 0.9887\n",
      " - lr: 0.02742 - momentum: 0.95 \n",
      "\n",
      "Testing loss: 0.36675751428604125, Val acc: 0.9392\n",
      "\n",
      "Epoch 50/50\n",
      "97/97 [==============================] - 13s 138ms/step - loss: 0.9001 - acc: 0.9903\n",
      " - lr: 0.01109 - momentum: 0.95 \n",
      "\n",
      "Testing loss: 0.36471467595100404, Val acc: 0.9391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "callbacks_list = [lr_manager, TestCallback((x_test, y_test))]\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model\n",
    "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 512),\n",
    "                                 samples_per_epoch = x_train.shape[0], nb_epoch = 50, \n",
    "                                 verbose=1, callbacks=callbacks_list)\n",
    "stop_time = time.time()\n",
    "\n",
    "# plot model history\n",
    "#plot_model_history(model_info)\n",
    "\n",
    "# compute test accuracy\n",
    "#print (\"Accuracy on test data is: %0.2f\"%accuracy(X_test, Y_test, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken =  680.3436517715454\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken = \" ,stop_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.91000000000001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aMERphS2Rv0G",
    "tCJM62tH-Hf8"
   ],
   "name": "90In24Under20.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
