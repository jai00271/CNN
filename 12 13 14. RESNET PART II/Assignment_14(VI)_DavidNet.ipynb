{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_14(VI)_DavidNet_Rohit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "v4jkwnVIs8zG",
        "X_VyxJ7bbJC0",
        "1tu-J4zbXU-W",
        "WYTv40qoMF_f",
        "kQnxlO9sD1yn",
        "XAf3IDGsLrsx",
        "XVY7W-Dq61-1",
        "krPN47N1LRzS",
        "n_ThvQ74L45Q",
        "ES3IqhGJa18k",
        "_wInflHNMy6q",
        "Esg4pencsqWQ",
        "3JPjTR4MMkVd",
        "0Oh2csQYQj3g",
        "d2vWUWAI_wKF",
        "xDF5RvR6_4w8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4jkwnVIs8zG",
        "colab_type": "text"
      },
      "source": [
        "### Load required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E14Gc5K0LHov",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "68fa58e2-c9ab-45a6-e778-e3f3f5a125be"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import regularizers, optimizers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.layers import MaxPooling2D\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VyxJ7bbJC0",
        "colab_type": "text"
      },
      "source": [
        "## Private Methods Sections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tu-J4zbXU-W",
        "colab_type": "text"
      },
      "source": [
        "###Define Cutout\n",
        "\n",
        "Visit readme file for more details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzhVX04EXTMx",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYTv40qoMF_f",
        "colab_type": "text"
      },
      "source": [
        "### Define LR  Finder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnPLBxMhjNAC",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "class LRFinder:\n",
        "    \"\"\"\n",
        "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
        "    See for details:\n",
        "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # Log the learning rate\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # Log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Check whether the loss got too large or NaN\n",
        "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.best_loss = loss\n",
        "\n",
        "        # Increase the learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n",
        "        num_batches = epochs * x_train.shape[0] / batch_size\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
        "\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs,\n",
        "                        callbacks=[callback])\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def find_generator(self, generator, start_lr, end_lr, epochs=1, steps_per_epoch=None, **kw_fit):\n",
        "            if steps_per_epoch is None:\n",
        "                try:\n",
        "                    steps_per_epoch = len(generator)\n",
        "                except (ValueError, NotImplementedError) as e:\n",
        "                    raise e('`steps_per_epoch=None` is only valid for a'\n",
        "                            ' generator based on the '\n",
        "                            '`keras.utils.Sequence`'\n",
        "                            ' class. Please specify `steps_per_epoch` '\n",
        "                            'or use the `keras.utils.Sequence` class.')\n",
        "            self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(steps_per_epoch))\n",
        "\n",
        "            # Save weights into a file\n",
        "            self.model.save_weights('tmp.h5')\n",
        "\n",
        "            # Remember the original learning rate\n",
        "            original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "            # Set the initial learning rate\n",
        "            K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "            callback = LambdaCallback(on_batch_end=lambda batch,\n",
        "                                      logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "            self.model.fit_generator(generator=generator,\n",
        "                                     epochs=epochs,\n",
        "                                     steps_per_epoch=steps_per_epoch,\n",
        "                                     callbacks=[callback],\n",
        "                                     **kw_fit)\n",
        "\n",
        "            # Restore the weights to the state before model fitting\n",
        "            self.model.load_weights('tmp.h5')\n",
        "\n",
        "            # Restore the original learning rate\n",
        "            K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
        "        \"\"\"\n",
        "        Plots the loss.\n",
        "        Parameters:\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale('log')\n",
        "\n",
        "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
        "        \"\"\"\n",
        "        Plots rate of change of the loss function.\n",
        "        Parameters:\n",
        "            sma - number of batches for simple moving average to smooth out the curve.\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "            y_lim - limits for the y axis.\n",
        "        \"\"\"\n",
        "        derivatives = self.get_derivatives(sma)[n_skip_beginning:-n_skip_end]\n",
        "        lrs = self.lrs[n_skip_beginning:-n_skip_end]\n",
        "        plt.ylabel(\"rate of loss change\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(lrs, derivatives)\n",
        "        plt.xscale('log')\n",
        "        plt.ylim(y_lim)\n",
        "\n",
        "    def get_derivatives(self, sma):\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
        "        return derivatives\n",
        "\n",
        "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
        "        derivatives = self.get_derivatives(sma)\n",
        "        print(derivatives)\n",
        "        best_der_idx = np.argmax(derivatives[n_skip_beginning:-n_skip_end])\n",
        "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQnxlO9sD1yn",
        "colab_type": "text"
      },
      "source": [
        "### Plot Model history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Txs63KuDzXg",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAf3IDGsLrsx",
        "colab_type": "text"
      },
      "source": [
        "### Define accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrJnVZdLLqiv",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVY7W-Dq61-1",
        "colab_type": "text"
      },
      "source": [
        "### Random crop defination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "416YYMS260oD",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def random_crop(x, random_crop_size = (32,32), sync_seed=None):\n",
        "    np.random.seed(sync_seed)\n",
        "    w, h = x.shape[1], x.shape[2]\n",
        "    rangew = (w - random_crop_size[0]) // 2\n",
        "    rangeh = (h - random_crop_size[1]) // 2\n",
        "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
        "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
        "    return x[:, offsetw:offsetw+random_crop_size[0], offseth:offseth+random_crop_size[1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOZxx9lG7yqo",
        "colab_type": "text"
      },
      "source": [
        "### Define One Cycle LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0wQ5EvI7wWF",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.metrics import binary_accuracy\n",
        "from keras import backend as K\n",
        "from keras.callbacks import *\n",
        "\n",
        "class OneCycleLR(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self,base_lr, max_lr, step_size, base_m, max_m, cyclical_momentum):\n",
        " \n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_m = base_m\n",
        "        self.max_m = max_m\n",
        "        self.cyclical_momentum = cyclical_momentum\n",
        "        self.step_size = step_size\n",
        "        \n",
        "        self.clr_iterations = 0.\n",
        "        self.cm_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "        \n",
        "    def clr(self):\n",
        "        \n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        \n",
        "        if cycle == 2:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)          \n",
        "            return self.base_lr-(self.base_lr-self.base_lr/100)*np.maximum(0,(1-x))\n",
        "        \n",
        "        else:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "            current_LR = self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0,(1-x))\n",
        "            # print('Current LR is' ,current_LR)\n",
        "            return current_LR\n",
        "    \n",
        "    def cm(self):\n",
        "        \n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        \n",
        "        if cycle == 2:\n",
        "            \n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1) \n",
        "            return self.max_m\n",
        "        \n",
        "        else:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "            return self.max_m - (self.max_m-self.base_m)*np.maximum(0,(1-x))\n",
        "        \n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "            \n",
        "        if self.cyclical_momentum == True:\n",
        "            if self.clr_iterations == 0:\n",
        "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
        "            else:\n",
        "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
        "            \n",
        "            \n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "        \n",
        "        if self.cyclical_momentum == True:\n",
        "            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "        \n",
        "        if self.cyclical_momentum == True:\n",
        "            K.set_value(self.model.optimizer.momentum, self.cm())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfZ5T2EobZnX",
        "colab_type": "text"
      },
      "source": [
        "## Implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krPN47N1LRzS",
        "colab_type": "text"
      },
      "source": [
        "###Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fidbp3f3LQpS",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "\n",
        "# Training parameters\n",
        "batch_size = 512  \n",
        "epochs = 150\n",
        "data_augmentation = True\n",
        "classes = 10\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ThvQ74L45Q",
        "colab_type": "text"
      },
      "source": [
        "### Test and Train data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdBoffNNq6ML",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "len_train, len_test = len(x_train), len(x_test)\n",
        "y_train = y_train.astype('int64').reshape(len_train)\n",
        "y_test = y_test.astype('int64').reshape(len_test)\n",
        "\n",
        "#train_mean = np.mean(x_train, axis=(0,1,2))\n",
        "train_mean = (0.4914, 0.4822, 0.4465)\n",
        "#train_std = np.std(x_train, axis=(0,1,2))\n",
        "train_std =  (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "normalize = lambda x: ((x - train_mean) / train_std).astype('float32') # todo: check here\n",
        "#pad4 = lambda x: np.pad(x, [(0, 0), (0, 0), (0, 0), (0, 0)], mode='reflect')\n",
        "\n",
        "x_train = normalize(x_train)\n",
        "x_test = normalize(x_test)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, classes)\n",
        "y_test = keras.utils.to_categorical(y_test, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES3IqhGJa18k",
        "colab_type": "text"
      },
      "source": [
        "### Davidnet implementation using keras "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aUmveIY2EDw",
        "colab_type": "code",
        "outputId": "fbaa7104-b4f1-4b1a-d319-9f0ed8905bc6",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "def Add_BasicLayer(input, num_filter = 12, dropout_rate = 0, add_pooling = False):\n",
        "    Conv2D_1_1 = Conv2D(int(num_filter), (3,3), use_bias=False, padding='same')(input)\n",
        "    BatchNorm = BatchNormalization()(Conv2D_1_1)\n",
        "    output = Activation('relu')(BatchNorm)\n",
        "    if add_pooling:\n",
        "      output = MaxPooling2D(pool_size=(2, 2))(output)\n",
        "    if dropout_rate>0:\n",
        "      output = Dropout(dropout_rate)(output)\n",
        "    return output\n",
        "c=64\n",
        "input = Input(shape=(32, 32, 3,))\n",
        "prep = Add_BasicLayer(input, num_filter = c, dropout_rate = 0, add_pooling = False)\n",
        "\n",
        "layer1_1 = Add_BasicLayer(prep, num_filter = c * 2, dropout_rate = 0, add_pooling = True)\n",
        "layer1_2 = Add_BasicLayer(layer1_1, num_filter = c * 2, dropout_rate = 0, add_pooling = False)\n",
        "layer1_3 = Add_BasicLayer(layer1_2, num_filter = c * 2, dropout_rate = 0, add_pooling = False)\n",
        "\n",
        "#merge1 = concatenate([layer1_1, layer1_3], name='merge1')\n",
        "#Conv2D_1 = Conv2D(32, (1,1), use_bias=False, padding='same')(merge1)\n",
        "merge1 = keras.layers.add([layer1_1, layer1_3])\n",
        "\n",
        "\n",
        "layer2 = Add_BasicLayer(merge1, num_filter = c * 4, dropout_rate = 0, add_pooling = True)\n",
        "\n",
        "layer3_1 = Add_BasicLayer(layer2, num_filter = c * 8, dropout_rate = 0, add_pooling = False)\n",
        "layer3_2 = Add_BasicLayer(layer3_1, num_filter = c * 8, dropout_rate = 0, add_pooling = False)\n",
        "layer3_3 = Add_BasicLayer(layer3_2, num_filter = c * 8, dropout_rate = 0, add_pooling = False)\n",
        "\n",
        "#merge2= concatenate([layer3_1, layer3_3], name='merge2')\n",
        "merge2 = keras.layers.add([layer3_1, layer3_3])\n",
        "\n",
        "output = AveragePooling2D(pool_size = (4,4))(merge2)\n",
        "output = Flatten()(output)\n",
        "output = Dense(10)(output)\n",
        "\n",
        "# output = Conv2D(10, kernel_size=(1,1))(merge2)\n",
        "# output = AveragePooling2D(pool_size = (4,4))(output)\n",
        "# output = Flatten()(output)\n",
        "output = Activation('softmax')(output)\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "\n",
        "#sgd = tf.keras.optimizers.SGD(nesterov=True)\n",
        "\n",
        "sgd = optimizers.SGD(nesterov=True)\n",
        "\n",
        "#loss = custom_loss()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0813 02:44:23.966725 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0813 02:44:23.985305 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0813 02:44:23.989744 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0813 02:44:24.018950 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0813 02:44:24.019970 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0813 02:44:24.526398 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0813 02:44:24.706867 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0813 02:44:25.312362 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0813 02:44:25.357352 140398385862528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wInflHNMy6q",
        "colab_type": "text"
      },
      "source": [
        "### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-tMvNRMjBr2",
        "colab_type": "code",
        "outputId": "a6af0131-9466-4beb-c668-cb061271c7a7",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 128)  73728       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 128)  147456      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 128)  147456      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 128)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 16, 16, 128)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 256)  294912      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 256)  1024        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 256)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 8, 8, 512)    1179648     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 8, 8, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 8, 8, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 8, 8, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 8, 8, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 8, 8, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 8, 8, 512)    2359296     activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 8, 8, 512)    2048        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 8, 8, 512)    0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 8, 8, 512)    0           activation_6[0][0]               \n",
            "                                                                 activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 2, 2, 512)    0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 2048)         0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           20490       flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 10)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 6,592,970\n",
            "Trainable params: 6,588,490\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esg4pencsqWQ",
        "colab_type": "text"
      },
      "source": [
        "### Finding best LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEdMLzNRPpBy",
        "colab_type": "code",
        "outputId": "a4246fef-9c98-43fa-ecfa-27581ac03550",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "from keras.callbacks import Callback, LambdaCallback\n",
        "import math\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='ADAM', metrics=['accuracy'])\n",
        "lr_finder = LRFinder(model)\n",
        "\n",
        "# Train a model with batch size 512 for 5 epochs\n",
        "# with learning rate growing exponentially from 0.0001 to 1(but I'll try higher values like 3,4)\n",
        "lr_finder.find(x_train, y_train, start_lr=0.0001, end_lr=50, batch_size=512, epochs=5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0813 02:44:26.233270 140398385862528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "50000/50000 [==============================] - 62s 1ms/step - loss: 1.2490 - acc: 0.5584\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 52s 1ms/step - loss: 1.1032 - acc: 0.6413\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 52s 1ms/step - loss: 0.8813 - acc: 0.6971\n",
            "Epoch 4/5\n",
            "12800/50000 [======>.......................] - ETA: 38s - loss: 1.4756 - acc: 0.5557"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICfdY0oGmty2",
        "colab_type": "text"
      },
      "source": [
        "#### Plot Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTmwyWEOmt7T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f7d17b4b-99d9-4ac7-d2ae-f7158bdbc06b"
      },
      "source": [
        "# Plot the loss, ignore 20 batches in the beginning and 5 in the end\n",
        "lr_finder.plot_loss(n_skip_beginning=1, n_skip_end=5) # changed n_skip_beginning from 20 to 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEOCAYAAACaQSCZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njuMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkScG3KIm6CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "51b2df6f-acba-484c-9d2a-6202785fa80a"
      },
      "source": [
        "# Plot rate of change of the loss\n",
        "# Ignore 20 batches in the beginning and 5 in the end\n",
        "# Smooth the curve using simple moving average of 20 batches\n",
        "# Limit the range for y axis to (-0.02, 0.01)\n",
        "lr_finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEOCAYAAADPIMPHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar1evM0PPtvn",
        "colab_type": "code",
        "outputId": "7e5a34bf-35d6-42de-ccf5-cc9aba37f5b2",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "bestLr = lr_finder.get_best_lr(sma=20)\n",
        "print(bestLr)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.08968098759651184, -0.0469321072101593, -0.03445432186126709, -0.03699927926063538, -0.02897096276283264, -0.025649797916412354, -0.027271300554275513, -0.02051992416381836, -0.019946295022964477, -0.0174816370010376, -0.014960986375808717, -0.020664727687835692, -0.017206913232803343, -0.012051379680633545, -0.014343839883804322, -0.013020187616348267, -0.009174537658691407, -0.009180635213851929, -0.009899705648422241, -0.010146087408065796, -0.009800279140472412, -0.007459920644760132, -0.007264232635498047, -0.004228365421295166, -0.008168172836303712, -0.0041477024555206295, -0.004015260934829712, -0.007906961441040038, -0.009109342098236084, -0.01267736554145813, -0.011653411388397216, -0.00012325048446655275, -0.007739925384521484, -0.002312976121902466, -0.01190827488899231, -0.009632813930511474, -0.007562482357025146, -0.005399453639984131, -0.011832207441329956, -0.006457477807998657, -0.00472765564918518, -0.0034248292446136474, -0.002699613571166992, -0.004192829132080078, -0.0015537619590759277, -0.008395951986312867, -0.0045224249362945555, -0.003991603851318359, -0.0006950557231903076, 0.004640942811965943, -0.005245429277420044, -0.013230603933334351, -0.005023843050003052, -0.00669867992401123, -0.002947998046875, -0.0039579510688781735, 0.0051362931728363035, -0.009032124280929565, 0.0008168578147888184, 0.0011381924152374268, -0.001956921815872192, -0.002962082624435425, -0.007161515951156616, -0.00860947072505951, 0.0017737925052642822, -0.007048720121383667, -0.0034311532974243162, -0.0015720367431640626, -0.008739998936653138, -0.004566740989685058, -0.0014361977577209474, 0.0032466888427734376, -0.0031592130661010744, -0.00040227174758911133, -0.000835159420967102, -0.0006908029317855835, -0.013067448139190673, -0.002177107334136963, -0.001014763116836548, -0.01298048198223114, -0.004936710000038147, -0.00599367618560791, -0.00016300976276397705, 0.00018559098243713378, -0.011386799812316894, 0.004588496685028076, -0.0051879346370697025, -0.002275407314300537, -0.0010046005249023438, -0.00835234522819519, -0.000454181432723999, -0.0013555437326431274, -0.004576379060745239, -0.0016534537076950072, 0.0027216106653213503, -0.003001609444618225, 0.0017996728420257568, 0.003332272171974182, -0.007343211770057678, 0.0032361119985580443, -0.003759044408798218, -0.005823686718940735, 0.004056230187416077, -0.0065668076276779175, 0.0028301090002059936, -0.001528710126876831, -0.0006083250045776367, -0.002551490068435669, 0.0017852157354354859, 0.0008627384901046753, 0.004218149185180664, 0.0032101184129714965, 0.00488162636756897, -0.0045525521039962765, -0.006234309077262879, -0.0006034791469573975, -0.0009217888116836547, 0.011311337351799011, 0.0008551418781280517, 0.008806437253952026, -0.0007991373538970947, 0.007991859316825866, -0.00809929370880127, 0.001503288745880127, -0.002332308888435364, -0.0033158838748931887, -0.0008969664573669434, -0.005539596080780029, 0.005782026052474976, 0.003801378607749939, -0.002805301547050476, -0.0012128353118896484, -9.527504444122315e-05, -0.0008329182863235473, 0.008819970488548278, 0.002997744083404541, 0.023087915778160096, -0.010263174772262573, 0.017690357565879822, 0.00037689805030822755, 0.03984058201313019, -0.001879364252090454, 0.02272736430168152, 0.02714592218399048, 0.02529548406600952, 0.021800798177719117, 0.005092912912368774, 0.019393676519393922, 0.026389998197555543, 0.01952648162841797, 0.0303135484457016, 0.027733343839645385, 0.04619322121143341, 0.04106023609638214, 0.01451871395111084, 0.042967033386230466, 0.005878978967666626, 0.01596776247024536, 0.015395981073379517, 0.009569096565246581, 0.0021192312240600588, 0.016592800617218018, 0.008517754077911378, 0.005222779512405395, -0.013485419750213622, -0.005869126319885254, 0.007100003957748413, -0.006105995178222657, -0.022788703441619873, -0.008748805522918702, -0.01833375096321106, -0.02443908452987671, -0.044390881061553956, -0.03274454474449158, -0.003849309682846069, -0.022031527757644654, -0.020408540964126587, -0.006813812255859375, -0.02716165781021118, -0.004351091384887695, -0.034722745418548584, -0.016921886801719667, -0.027572092413902283, -0.024469402432441712, -0.007619753479957581, -0.02098618745803833, -0.012182283401489257, -0.0111025869846344, -0.011891525983810425, -0.011306992173194886, -0.014980307221412659, -0.006993207335472107, -0.0021378040313720704, -0.002128636837005615, -0.018310898542404176, -0.021495634317398073, -0.010425743460655213, -0.010258901119232177, -0.006849724054336548, -0.012499776482582093, -0.006710016727447509, -0.002296748757362366, -0.006618902087211609, -0.0061339884996414185, -0.00816975235939026, -0.005958884954452515, 0.001268640160560608, -0.005734607577323914, -0.005117523670196533, -0.001048162579536438, -0.00014539062976837158, 0.0013074517250061036, -0.008734869956970214, -0.00822291374206543, -0.0002984523773193359, -0.0007551491260528564, -0.0025932133197784425, -0.008296999335289, -0.0029450923204421995, -0.004593750834465027, 0.00041382312774658204, -0.0038856327533721923, 0.000965958833694458, -0.0007344841957092285, 0.007674744725227356, 0.0046856701374053955, -0.002813553810119629, 0.0026261359453201294, 0.0014680176973342896, -0.002141481637954712, -0.0060883045196533205, -0.009729179739952087, -0.004427078366279602, -0.003065359592437744, -0.00856059193611145, -0.00822111964225769, -0.007314929366111755, 0.003816491365432739, -0.003003960847854614, 0.001770329475402832, 0.0032606959342956544, -0.0024214446544647216, 0.0016761690378189087, -0.001626250147819519, -0.011353489756584168, -0.0012945353984832764, -0.00498167872428894, -0.0032498657703399656, -0.004018926620483398, -0.00047562718391418456, 0.0008139729499816895, 0.004217565059661865, 0.003342941403388977, 0.0006220072507858276, 0.011268717050552369, 0.004750171303749084, 0.005885747075080871, -0.0027547985315322875, 0.0038908541202545164, -0.000870603322982788, -0.0010043084621429443, 0.0003510653972625732, -0.00044040679931640626, 0.005563268065452575, 0.006854423880577087, 0.009575435519218444, 0.007368108630180359, 0.008604049682617188, 0.017612966895103454, 0.006780856847763061, 0.017369651794433595, 0.0036019295454025268, 0.01041368842124939, 0.007452183961868286, 0.006567174196243286, 0.01988174021244049, 0.024386659264564514, 0.014304268360137939, 0.0210009902715683, 0.01758763790130615, 0.023213881254196166, 0.024732428789138793, 0.01322912871837616, 0.020349401235580444, 0.008009666204452514, 0.012473770976066589, 0.01033552885055542, 0.00904276967048645, -0.0069619804620742794, 0.008908271789550781, 0.03258610367774963, 0.05254619121551514, 0.08080416321754455, 0.04535928070545196, 0.03592748641967773, 0.005516141653060913, 0.005222523212432861, 0.03138368129730225, 0.10352610349655152]\n",
            "0.43803126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JPjTR4MMkVd",
        "colab_type": "text"
      },
      "source": [
        "### Set best starting LR and Compile model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMSFVdupo-wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "\n",
        "sgd = SGD(lr=bestLr, momentum=0.9, decay=5e-4, nesterov=False)\n",
        "K.set_value(model.optimizer.lr, 0.5 * K.get_value(sgd.lr))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oh2csQYQj3g",
        "colab_type": "text"
      },
      "source": [
        "### Define CLR parameters and model fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_4UxJVru-Ri",
        "colab_type": "code",
        "outputId": "f2096cef-c128-4e56-bdb9-a3f4187e5ec2",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from keras.utils import Sequence\n",
        "\n",
        "\n",
        "# # Prepare model model saving directory.\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "# #model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "# if not os.path.isdir(save_dir):\n",
        "#     os.makedirs(save_dir)\n",
        "    \n",
        "    \n",
        "#filepath = os.path.join(save_dir, model_name)\n",
        "filepath = \"model_accuracy_Assignment_14.best.hdf5\"\n",
        "\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint (\n",
        "                             filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50,min_delta=0.01)\n",
        "\n",
        "\n",
        "\n",
        "# Cyclic LR parameters\n",
        "max_lr = bestLr\n",
        "base_lr = bestLr//5\n",
        "max_m = 0.95\n",
        "base_m = 0.90\n",
        "cyclical_momentum = True\n",
        "augment = True\n",
        "cycles = 5\n",
        "\n",
        "iterations = round(len(x_train)/batch_size*epochs)\n",
        "iterations = list(range(0,iterations+1))\n",
        "step_size = len(iterations)/(cycles)\n",
        "\n",
        "clr =  OneCycleLR(base_lr=base_lr,\n",
        "                max_lr=max_lr,\n",
        "                step_size=step_size,\n",
        "                max_m=max_m,\n",
        "                base_m=base_m,\n",
        "                cyclical_momentum=cyclical_momentum)\n",
        "\n",
        "                         \n",
        "                       \n",
        "\n",
        "#callbacks = [checkpoint,clr_triangular] # Last working\n",
        "\n",
        "callbacks =  [es , clr]\n",
        "\n",
        "# get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False)\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             #width_shift_range=0.1, \n",
        "                             #height_shift_range=0.1,\n",
        "                             horizontal_flip=0.5,\n",
        "                            preprocessing_function=get_random_eraser(p=0.3,s_l=0.02, s_h=0.3, r_1=0.3, r_2=1/0.3, v_l=0, v_h=1, pixel_level=False))\n",
        "\n",
        "    \n",
        "# # Compute quantities required for featurewise normalization\n",
        "# # (std, mean, and principal components if ZCA whitening is applied).\n",
        "# datagen.fit(x_train)\n",
        "    \n",
        "   \n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model_info = model.fit_generator(datagen.flow(random_crop(x_train), y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                    validation_steps = x_train.shape[0] // batch_size,\n",
        "                    validation_data=(random_crop(x_test), y_test),\n",
        "                    epochs=epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "  \n",
        " \n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy\n",
        "print (\"Accuracy on test data is: %0.2f\"%accuracy(x_test, y_test, model))\n",
        "\n",
        "print(\"--- Total %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "97/97 [==============================] - 56s 574ms/step - loss: 1.6215 - acc: 0.4251 - val_loss: 3.0181 - val_acc: 0.1000\n",
            "Epoch 2/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 1.1612 - acc: 0.6052 - val_loss: 2.6711 - val_acc: 0.1000\n",
            "Epoch 3/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 1.1094 - acc: 0.6551 - val_loss: 2.6598 - val_acc: 0.1667\n",
            "Epoch 4/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.8384 - acc: 0.7282 - val_loss: 1.6390 - val_acc: 0.4306\n",
            "Epoch 5/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.7169 - acc: 0.7677 - val_loss: 1.2175 - val_acc: 0.6345\n",
            "Epoch 6/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.6382 - acc: 0.7923 - val_loss: 1.8183 - val_acc: 0.5083\n",
            "Epoch 7/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.5224 - acc: 0.8242 - val_loss: 1.1129 - val_acc: 0.6787\n",
            "Epoch 8/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.4571 - acc: 0.8447 - val_loss: 0.8725 - val_acc: 0.7458\n",
            "Epoch 9/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 0.4382 - acc: 0.8536 - val_loss: 0.9755 - val_acc: 0.7509\n",
            "Epoch 10/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.3520 - acc: 0.8786 - val_loss: 0.6791 - val_acc: 0.7701\n",
            "Epoch 11/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.2896 - acc: 0.9001 - val_loss: 0.6033 - val_acc: 0.8115\n",
            "Epoch 12/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.2506 - acc: 0.9130 - val_loss: 0.6558 - val_acc: 0.8062\n",
            "Epoch 13/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 0.2213 - acc: 0.9230 - val_loss: 0.8492 - val_acc: 0.7666\n",
            "Epoch 14/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.2026 - acc: 0.9297 - val_loss: 0.7888 - val_acc: 0.7593\n",
            "Epoch 15/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.1812 - acc: 0.9378 - val_loss: 0.6406 - val_acc: 0.8073\n",
            "Epoch 16/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.1629 - acc: 0.9439 - val_loss: 0.7170 - val_acc: 0.7995\n",
            "Epoch 17/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 0.1432 - acc: 0.9500 - val_loss: 0.7776 - val_acc: 0.7766\n",
            "Epoch 18/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.1208 - acc: 0.9584 - val_loss: 0.5006 - val_acc: 0.8557\n",
            "Epoch 19/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.1064 - acc: 0.9626 - val_loss: 0.4657 - val_acc: 0.8659\n",
            "Epoch 20/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.1004 - acc: 0.9651 - val_loss: 0.4363 - val_acc: 0.8661\n",
            "Epoch 21/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0913 - acc: 0.9689 - val_loss: 0.4430 - val_acc: 0.8650\n",
            "Epoch 22/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 0.0790 - acc: 0.9731 - val_loss: 0.6750 - val_acc: 0.8170\n",
            "Epoch 23/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0731 - acc: 0.9747 - val_loss: 0.5372 - val_acc: 0.8661\n",
            "Epoch 24/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 0.0714 - acc: 0.9759 - val_loss: 0.5160 - val_acc: 0.8629\n",
            "Epoch 25/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0651 - acc: 0.9772 - val_loss: 0.4960 - val_acc: 0.8645\n",
            "Epoch 26/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0625 - acc: 0.9788 - val_loss: 0.5993 - val_acc: 0.8499\n",
            "Epoch 27/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0540 - acc: 0.9818 - val_loss: 0.6695 - val_acc: 0.8526\n",
            "Epoch 28/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0543 - acc: 0.9814 - val_loss: 0.5398 - val_acc: 0.8563\n",
            "Epoch 29/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0488 - acc: 0.9836 - val_loss: 0.5624 - val_acc: 0.8671\n",
            "Epoch 30/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0456 - acc: 0.9849 - val_loss: 0.4824 - val_acc: 0.8843\n",
            "Epoch 31/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0420 - acc: 0.9858 - val_loss: 0.6204 - val_acc: 0.8686\n",
            "Epoch 32/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0378 - acc: 0.9873 - val_loss: 0.6082 - val_acc: 0.8655\n",
            "Epoch 33/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0377 - acc: 0.9873 - val_loss: 0.5303 - val_acc: 0.8817\n",
            "Epoch 34/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0337 - acc: 0.9894 - val_loss: 0.4602 - val_acc: 0.8850\n",
            "Epoch 35/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 0.0339 - acc: 0.9888 - val_loss: 0.5305 - val_acc: 0.8886\n",
            "Epoch 36/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0294 - acc: 0.9903 - val_loss: 0.5278 - val_acc: 0.8747\n",
            "Epoch 37/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 0.0291 - acc: 0.9906 - val_loss: 0.4654 - val_acc: 0.8952\n",
            "Epoch 38/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0288 - acc: 0.9904 - val_loss: 0.5416 - val_acc: 0.8827\n",
            "Epoch 39/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0249 - acc: 0.9918 - val_loss: 0.4579 - val_acc: 0.8933\n",
            "Epoch 40/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0256 - acc: 0.9915 - val_loss: 0.4267 - val_acc: 0.8943\n",
            "Epoch 41/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0229 - acc: 0.9923 - val_loss: 0.4584 - val_acc: 0.8921\n",
            "Epoch 42/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0211 - acc: 0.9929 - val_loss: 0.5158 - val_acc: 0.8850\n",
            "Epoch 43/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0190 - acc: 0.9936 - val_loss: 0.4479 - val_acc: 0.8938\n",
            "Epoch 44/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0179 - acc: 0.9943 - val_loss: 0.4236 - val_acc: 0.9013\n",
            "Epoch 45/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0192 - acc: 0.9937 - val_loss: 0.4204 - val_acc: 0.9024\n",
            "Epoch 46/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0182 - acc: 0.9939 - val_loss: 0.4623 - val_acc: 0.9011\n",
            "Epoch 47/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0165 - acc: 0.9945 - val_loss: 0.4531 - val_acc: 0.8980\n",
            "Epoch 48/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0150 - acc: 0.9952 - val_loss: 0.4034 - val_acc: 0.9047\n",
            "Epoch 49/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.4101 - val_acc: 0.9079\n",
            "Epoch 50/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0154 - acc: 0.9948 - val_loss: 0.4322 - val_acc: 0.9075\n",
            "Epoch 51/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0146 - acc: 0.9951 - val_loss: 0.4073 - val_acc: 0.9111\n",
            "Epoch 52/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.3948 - val_acc: 0.9108\n",
            "Epoch 53/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0120 - acc: 0.9961 - val_loss: 0.3824 - val_acc: 0.9101\n",
            "Epoch 54/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0113 - acc: 0.9966 - val_loss: 0.3839 - val_acc: 0.9105\n",
            "Epoch 55/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0125 - acc: 0.9956 - val_loss: 0.3805 - val_acc: 0.9133\n",
            "Epoch 56/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.3823 - val_acc: 0.9136\n",
            "Epoch 57/150\n",
            "97/97 [==============================] - 53s 551ms/step - loss: 0.0109 - acc: 0.9962 - val_loss: 0.3739 - val_acc: 0.9137\n",
            "Epoch 58/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0101 - acc: 0.9967 - val_loss: 0.3719 - val_acc: 0.9145\n",
            "Epoch 59/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.3677 - val_acc: 0.9158\n",
            "Epoch 60/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0109 - acc: 0.9964 - val_loss: 0.3653 - val_acc: 0.9159\n",
            "Epoch 61/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0110 - acc: 0.9966 - val_loss: 0.3660 - val_acc: 0.9167\n",
            "Epoch 62/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0102 - acc: 0.9967 - val_loss: 0.3662 - val_acc: 0.9164\n",
            "Epoch 63/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0101 - acc: 0.9969 - val_loss: 0.3666 - val_acc: 0.9165\n",
            "Epoch 64/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0100 - acc: 0.9966 - val_loss: 0.3665 - val_acc: 0.9166\n",
            "Epoch 65/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0100 - acc: 0.9964 - val_loss: 0.3664 - val_acc: 0.9164\n",
            "Epoch 66/150\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 0.0102 - acc: 0.9967 - val_loss: 0.3662 - val_acc: 0.9165\n",
            "Epoch 67/150\n",
            "97/97 [==============================] - 54s 552ms/step - loss: 0.0100 - acc: 0.9967 - val_loss: 0.3667 - val_acc: 0.9165\n",
            "Epoch 68/150\n",
            "50/97 [==============>...............] - ETA: 24s - loss: 0.0087 - acc: 0.9972"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1e64feae2778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                     callbacks=callbacks)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZjIdr8ImYiw",
        "colab_type": "text"
      },
      "source": [
        "## Stats and Plot Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2vWUWAI_wKF",
        "colab_type": "text"
      },
      "source": [
        "### One Cycle LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ipScVg_u7Y",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(clr.history['iterations'], clr.history['lr'])\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"One Cycle Policy\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDF5RvR6_4w8",
        "colab_type": "text"
      },
      "source": [
        "### Cyclic Momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJoEb62q_33n",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(clr.history['iterations'], clr.history['momentum'])\n",
        "plt.xlabel('Training Iterations')\n",
        "plt.ylabel('Momentum')\n",
        "plt.title(\"One Cycle Policy\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjkgPPUg1nrW",
        "colab_type": "text"
      },
      "source": [
        "### Print Confusion matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NNTkica268g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFyUVP2OmbYl",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "print('\\n', sklearn.metrics.classification_report(np.where(ytest > 0)[1], np.argmax(Y_pred, axis=1), target_names=list(dict_characters.values())), sep='')    \n",
        "    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "    Y_true = np.argmax(ytest,axis = 1) \n",
        "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "    plot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpH9-5d91vS_",
        "colab_type": "text"
      },
      "source": [
        "### Learning Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-99a64YD17wX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotKerasLearningCurve():\n",
        "    plt.figure(figsize=(10,5))\n",
        "    metrics = np.load('logs.npy')[()]\n",
        "    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
        "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
        "        l = np.array(metrics[k])\n",
        "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
        "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
        "        y = l[x]\n",
        "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
        "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
        "    plt.legend(loc=4)\n",
        "    plt.axis([0, None, None, None]);\n",
        "    plt.grid()\n",
        "    plt.xlabel('Number of epochs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQHr21tz1yld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotKerasLearningCurve()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQMi49fAVpGt",
        "colab_type": "text"
      },
      "source": [
        "### observation :\n",
        "\n",
        "last best conf : cycle : 5 and base_lr =bestLr//5 best:88 in 30 epochs\n",
        "\n",
        "Round2: \n",
        "\n",
        "1. Latest change : cycle :  20 and base_lr =bestLr//10\n",
        "2. Changed random eraser size and reduce it to .3 from .5\n",
        "3. LR finder is run for 5 epochs now\n",
        "4. Removed most of the hyperparameter from fit()\n",
        "5. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}