## Advanced Convolutions

*Dilated Convolution a.k.a Atrous Convolution*

Dilated convolutions helps us in increasing the receptive field without adding any parameters to our network. The key application the dilated convolution authors have in mind is dense prediction: vision applications where the predicted object that has similar size and structure to the input image. Dilated convolutions are used in Image segmentations, Audio generations, Machine translations, etc. In many such applications one wants to integrate information from different spatial scales and balance two properties:

1. local, pixel-level accuracy, such as precise detection of edges, and
2. integrating knowledge of the wider, global context

In simple words,

**When l=1, it is standard convolution.**

**When l>1, it is dilated convolution.**


  ![alt text](https://cdn-images-1.medium.com/max/2000/0*oX5IPr7TlVM2NpEU.gif)

**Standard Convolution (l=1)**

![alt text](https://cdn-images-1.medium.com/max/2000/0*3cTXIemm0k3Sbask.gif)

**Dilated Convolution (l=2)**

We can see that **the receptive field is larger** compared with the standard one.

![alt text](https://cdn-images-1.medium.com/max/2000/1*tnDNIyPePgHvb8JIx8SbqA.png)

​                            (a)                                                            (b)                                              (c)

In above image we see that Systematic dilation supports exponential expansion of the receptive field without loss of
resolution or coverage. (a) F1 is produced from F0 by a 1-dilated convolution; each element in F1 has a receptive field of 3×3. (b) F2 is produced from F1 by a 2-dilated convolution; each element in F2 has a receptive field of 7×7. (c) F3 is produced from F2 by a 4-dilated convolution; each element in F3 has a receptive field of 15×15. The number of parameters associated with each layer is identical. The receptive field grows exponentially while the number of parameters grows linearly. easy to see that the size of the receptive field of each element in Fi+1 is (2i+2 − 1)×(2i+2 − 1).
The receptive field is a square of exponentially increasing size.

- Figure (a) is a 1-dilated 3x3 convolution filter. In other words, it's a standard 3x3 convolution filter.
- Figure (b) is a 2-dilated 3x3 convolution filter. The red dots are where the weights are and everywhere else is 0. In other words, it's a **5x5 convolution filter with 9 non-zero weights and everywhere else 0**. The receptive field in this case is 7x7 because each unit in the previous output has a receptive field of 3x3. The highlighted portions in blue show the receptive field and **NOT** the convolution filter (you could see it as a convolution filter if you wanted to but it's not helpful).
- Figure (c) is a 4-dilated 3x3 convolution filter. It's a **9x9 convolution filter with 9 non-zeros weights and everywhere else 0**. From (b), we have it that each unit now has a 7x7 receptive field, and hence you can see a 7x7 blue portion around each red dot.

------

*DECONVOLUTION or Fractionally Strided OR Transpose Convolution*

The improvement of resolution of images or other data by a mathematical algorithm designed to separate the information from artefacts which result from the method of collecting it. Deconvolution layer is a very unfortunate name and should rather be called a transposed convolutional layer.

Visually, for a transposed convolution with stride one and no padding, we just pad the original input (blue entries) with zeroes (white entries) (Figure 1).

![alt text](https://i.stack.imgur.com/YyCu2.gif)

In case of stride two and padding, the transposed convolution would look like this (Figure 2):

![alt text](https://i.stack.imgur.com/f2RiP.gif)

You can find more (great) visualisations of convolutional arithmetics [here](https://github.com/vdumoulin/conv_arithmetic).

Now this introduces checker board issue. Lets understand what is checker board issue?

Checker Board Issue:

When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. It’s more obvious in some cases than others, but a large fraction of recent models exhibit this behavior.

![alt text](http://distill.pub/2016/deconv-checkerboard/thumbnail.jpg)

Now if you notice above, you can see small checkboxes. This is checkerboard issue. 

![alt text](https://jie-tao.com/wp-content/uploads/2019/02/checkerboard-2.jpg)

------

*Spatial Separable Convolutions*

With growing usage of mobiles around the world we need a better convolutions technique which needs less computation and can be run on mobile devices. One can perform separable convolution spatially (spatially separable convolution) or depthwise (depthwise separable convolution). Separable Convolutions are used in some neural net architectures, such as the [MobileNet](<https://arxiv.org/abs/1704.04861>) (MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications)

![alt text](https://cdn-images-1.medium.com/max/2000/1*o3mKhG3nHS-1dWa_plCeFw.png)

In this convolution instead of operating with 3x3 kernel over input image, we first convolve with 3x1 kernel and after getting intermediate result we again convolve with 1x3 kernel. Now by doing spatial separable convolutions we are only performing 6 multiplications instead of 9. 

![alt text](https://cdn-images-1.medium.com/max/2000/1*7OF9tl-oRpDK_S3z-AMwnA.png)

Above is another example explaining the same.

Disadvantage:

The main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels.

------

*Depthwise Separable Convolution*

Depthwise Separable Convolution is used to reduce the model size and complexity. It is particularly useful for mobile and embedded vision applications.

- Smaller model size: Fewer number of parameters
- Smaller complexity: Fewer Multiplications and Additions (Multi-Adds)

But before jumping to Depthwise Separable Convolution, lets understand how Normal and Depthwise convolutions works.

![Normal Convolution](https://cdn-images-1.medium.com/max/2000/1*fgYepSWdgywsqorf3bdksg.png)

​	                                                                        Normal convolution

Above is an example of normal convolution where our filter size is 5x5x3(height x length x channel) which is convolving on 12 x 12 x 3 image(height x width x channel) giving us a result of 8x8x1. This means our result is 1 image of 8X8 size and containing 1 channel. Now you might be thinking why are we reducing it to 1 channel, what if I want more channels.

![alt text](https://cdn-images-1.medium.com/max/2000/1*XloAmCh5bwE4j1G7yk5THw.png)

​                                                      Normal convolution with 8x8x256 output

Well, we can create 256 kernels to create 256 images of 8x8x1 dimensions, then stack them up together to create a 8x8x256 image output.

*Depth Wise Convolution*

![alt text](https://cdn-images-1.medium.com/max/2000/1*Esdvt3HLoEQFen94x29Z0A.png)

Now in depth wise convolution, Filters and image have been broken into three different channels and then convolved separately and stacked thereafter. See above, we have 3 channel filter and 3 channel image. What we do is — break the filter and image into three different channels and then convolve the corresponding image with corresponding channel and then stack them back. Although parameters are remaining same, this convolution gives you three output channels with only one 3-channel filter while, you would require three 3-channel filters if you would use normal convolution.

Now lets read about Depth wise separable convolutions.

![alt text](https://cdn-images-1.medium.com/max/2000/1*Voah8cvrs7gnTDf6acRvDw.png)

Now you can say that Depthwise separable convolutions = Depthwise convolutions + Pointwise Convolutions

1. **Depthwise convolution** is the **channel-wise DK×DK spatial convolution**. Suppose in the figure above, we have 5 channels, then we will have 5 DK×DK spatial convolution.
2. **Pointwise convolution** actually is the **1×1 convolution** to change the dimension.

Advantage of Depthwise Separable convolution over normal convolutions.

 Computation of normal 2D:

![alt text](https://cdn-images-1.medium.com/max/2000/1*QsxXZN0Pq9ZL_lckPtW6pw.png)

Let's say we have 7x7x3 input image and we are convolving with a 3x3x3x1(height = 3, width = 3, channel=3, no. of kernel =1) filter on it. Now as we know the result will be 5x5x1. Let's check our calculation here:

3x3x3x1x5x5x1 = 675

![alt text](https://cdn-images-1.medium.com/max/2000/1*VzwZ3Igv9KL-TCV-ZkB4Dg.png)

Now let's us do the same with with a 3x3x3x128 filter on it. Now as we know the result will be 5x5x128. Let's check our calculation here:

3x3x3x128x5x5 = 86400

 Computation of Depthwise separable convolution:

![alt text](https://cdn-images-1.medium.com/max/2000/1*TT2ldUUD2JM2eSayC3i9Lw.png)

Our input image is same as 7x7x3 and we convolve with 3x3x1x3(l = 3, w= 3, c=1, no of kernels = 3) kernels on it giving us an output image of 5x5x3

![alt text](https://cdn-images-1.medium.com/max/2000/1*L7fGwFxJKBUVOiytuIpVEg.png)

Now take this output image of 5x5x3 and convolve with 1x1x3x128 filter. Our output image will be 5x5x1 which is same as normal convolution. Let's check our calculation now:

3x3x1 x3 x5x5 + 1x1x3x128x5x5 = 675 + 9600= 10275

So, what’s the advantage of doing depthwise separable convolutions? Efficiency! One needs much less operations for depthwise separable convolutions compared to 2D convolutions. 

According to [MobileNets White Paper](<https://arxiv.org/pdf/1704.04861.pdf>)

```
The standard convolutional layer is parameterized by convolution kernel K of size DK × DK × M × N where DK is the spatial dimension of the kernel assumed to be square and M is number of input channels and N is the number of output channels as defined previously.

Standard convolutions have the computational cost of:
DK · DK · M · N · DF · DF

where the computational cost depends multiplicatively on the number of input channels M, the number of output channels N the kernel size Dk × Dk and the feature map size DF × DF.

Depthwise separable convolutions cost:
DK · DK · M · DF · DF + M · N · DF · DF

which is the sum of the depthwise and 1 × 1 pointwise convolutions.

MobileNet uses 3 × 3 depthwise separable convolutions which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy
```

![alt text](https://image.slidesharecdn.com/mobilenetv1v2slide-180823083642/95/mobilenetv1-v2-slide-12-638.jpg?cb=1535013428)

Above is the structure of standard convolutions vs depthwise separable convolutions

Disadvantage:

The depthwise separable convolutions reduces the number of parameters in the convolution. As such, for a small model, the model capacity may be decreased significantly if the 2D convolutions are replaced by depthwise separable convolutions. As a result, the model may become sub-optimal. However, if properly used, depthwise separable convolutions can give you the efficiency without dramatically damaging your model performance.

------

*Grouped convolution*

Grouped convolution was introduced to efficiently make use of multiple GPU's ad perform parallel processing.

![alt text](https://cdn-images-1.medium.com/max/2000/1*N4Ap65SA9rNf5HRrMhLf-Q.png)

The AlexNet above shows two separate convolution paths at most of the layers. It’s doing model-parallelization across two GPUs. If we notice, there are 2 separate task happening and later we combine the output of both.

As we already know normal convolution works by in traditional way of kernels moving over input images like below:

![alt text](https://cdn-images-1.medium.com/max/2000/1*oFVlkvZp848nh-QoD3pREw.png)

In above example, the input layer of size (7 x 7 x 3) is transformed into the output layer of size (5 x 5 x 128) by applying 128 filters (each filter is of size 3 x 3 x 3). Or in general case, the input layer of size (Hin x Win x Din) is transformed into the output layer of size (Hout x Wout x Dout) by applying Dout kernels (each is of size h x w x Din).

In grouped convolution, the filters are separated into different groups. Each group is responsible for a conventional 2D convolutions with certain depth. The following examples can make this clearer.

![alt text](https://cdn-images-1.medium.com/max/2000/1*dBrsVP0nt_PrBlICSBTttg.png)

Above is the illustration of grouped convolution with 2 filter groups. In each filter group, the depth of each filter is only half of the that in the nominal 2D convolutions. They are of depth Din / 2. Each filter group contains Dout /2 filters. The first filter group (red) convolves with the first half of the input layer ([:, :, 0:Din/2]), while the second filter group (blue) convolves with the second half of the input layer ([:, :, Din/2:Din]). As a result, each filter group creates Dout/2 channels. Overall, two groups create 2 x Dout/2 = Dout channels. We then stack these channels in the output layer with Dout channels.

*Bonus*:

## Tips For Using Dropout

The original paper on Dropout provides experimental results on a suite of standard machine learning problems. As a result they provide a number of useful heuristics to consider when using dropout in practice.

- Generally, use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.
- Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.
- Use dropout on incoming (visible) as well as hidden units. Application of dropout at each layer of the network has shown good results.
- Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.
- Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 has been shown to improve results.

Reference:

[Dilated Convo White paper](https://arxiv.org/pdf/1511.07122.pdf)

[DECONVOLUTION or Fractionally Strided OR Transpose Convolution](https://datascience.stackexchange.com/a/12110/74860)

[Checker Board Issue](https://distill.pub/2016/deconv-checkerboard/)

[Depthwise-Separable-Convolution](<https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69>)

[Separable Convolution](<https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215>)

[Grouped convolution](<https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215>)

[DropOut](<https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/>)

