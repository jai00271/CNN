{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/glove-global-vectors-for-word-representation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimdb_dir = '../input/keras-imdb/aclImdb_v1/aclImdb' # Data directory\ntrain_dir = os.path.join(imdb_dir, 'train') # Get the path of the train set\n\n# Setup empty lists to fill\nlabels = []\ntexts = []\n\n# First go through the negatives, then through the positives\nfor label_type in ['neg', 'pos']:\n    # Get the sub path\n    dir_name = os.path.join(train_dir, label_type)\n    print('loading ',label_type)\n    # Loop over all files in path\n    for fname in tqdm(os.listdir(dir_name)):\n        \n        # Only consider text files\n        if fname[-4:] == '.txt':\n            # Read the text file and put it in the list\n            f = open(os.path.join(dir_name, fname))\n            texts.append(f.read())\n            f.close()\n            # Attach the corresponding label\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should have 25,000 texts and labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(labels), len(texts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Half of the reviews are positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nnp.mean(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at a positive review:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Label',labels[24002])\nprint(texts[24002])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And a negative review:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Label',labels[1])\nprint(texts[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenizing text**\nComputers can not work with words directly. To them, a word is just a meaningless row of characters. To work with words, we need to turn words into so called 'Tokens'. A token is a number that represents that word. Each word gets assigned a token. Tokens are usually assigned by word frequency. The most frequent words like 'a' or 'the' get tokens like 1 or 2 while less often used words like 'profusely' get assigned very high numbers.\n\nWe can tokenize text directly with Keras. When we tokenize text, we usually choose a maximum number of words we want to consider, our vocabulary so to speak. This prevents us from assigning tokens to words that are hardly ever used, mostly because of typos or because they are not actual words or because they are just very uncommon. This prevents us from over fitting to texts that contain strange words or wired spelling errors. Words that are beyond that cutoff point get assigned the token 0, unknown."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nimport numpy as np\n\nmax_words = 10000 # We will only consider the 10K most used words in this dataset\n\ntokenizer = Tokenizer(num_words=max_words) # Setup\ntokenizer.fit_on_texts(texts) # Generate tokens by counting frequency\nsequences = tokenizer.texts_to_sequences(texts) # Turn text into sequence of numbers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Token for \"the\"',word_index['the'])\nprint('Token for \"Movie\"',word_index['movie'])\nprint('Token for \"generator\"',word_index['generator'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the first 10 words of the sequence tokenized\nsequences[24002][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nmaxlen = 100 # Make all sequences 100 words long\ndata = pad_sequences(sequences, maxlen=maxlen)\nprint(data.shape) # We have 25K, 100 word sequences now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.asarray(labels)\n\n# Shuffle data\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\ntraining_samples = 20000  # We will be training on 10K samples\nvalidation_samples = 5000  # We will be validating on 10000 samples\n\n# Split data\nx_train = data[:training_samples]\ny_train = labels[:training_samples]\nx_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that training your own embeddings is prone to over fitting. As you can see our model archives 100% accuracy on the training set but only 83% accuracy on the validation set. A clear sign of over fitting. In practice it is therefore quite rare to train new embeddings unless you have a massive dataset. Much more commonly, pre trained embeddings are used. A common pretrained embedding is GloVe, Global Vectors for Word Representation. It has been trained on billions of words from Wikipedia and the Gigaword 5 dataset, more than we could ever hope to train from our movie reviews. After downloading the GloVe embeddings from the GloVe website we can load them into our model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_dir = '../input/glove-global-vectors-for-word-representation' # This is the folder with the dataset\n\nprint('Loading word vectors')\nembeddings_index = {} # We create a dictionary of word -> embedding\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt')) # Open file\n\n# In the dataset, each line represents a new word embedding\n# The line starts with the word and the embedding values follow\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0] # The first value is the word, the rest are the values of the embedding\n    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n    embeddings_index[word] = embedding # Add embedding to our embedding dictionary\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a matrix of all embeddings\nall_embs = np.stack(embeddings_index.values())\nemb_mean = all_embs.mean() # Calculate mean\nemb_std = all_embs.std() # Calculate standard deviation\nemb_mean,emb_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 100 # We now use larger embeddings\n\nword_index = tokenizer.word_index\nnb_words = min(max_words, len(word_index)) # How many words are there actually\n\n# Create a random matrix with the same mean and std as the embeddings\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n\n# The vectors need to be in the same position as their index. \n# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n\n# Loop over all words in the word index\nfor word, i in word_index.items():\n    # If we are above the amount of words we want to use we do nothing\n    if i >= max_words: \n        continue\n    # Get the embedding vector for the word\n    embedding_vector = embeddings_index.get(word)\n    # If there is an embedding vector, put it in the embedding matrix\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights = [embedding_matrix], trainable = False))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Demo on a positive text\nmy_text = 'I love dogs. Dogs are the best. They are lovely, cuddly animals that only want the best for humans.'\n\nseq = tokenizer.texts_to_sequences([my_text])\nprint('raw seq:',seq)\nseq = pad_sequences(seq, maxlen=maxlen)\nprint('padded seq:',seq)\nprediction = model.predict(seq)\nprint('positivity:',prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Demo on a negative text\nmy_text = 'The bleak economic outlook will force many small businesses into bankruptcy.'\n\nseq = tokenizer.texts_to_sequences([my_text])\nprint('raw seq:',seq)\nseq = pad_sequences(seq, maxlen=maxlen)\nprint('padded seq:',seq)\nprediction = model.predict(seq)\nprint('positivity:',prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}