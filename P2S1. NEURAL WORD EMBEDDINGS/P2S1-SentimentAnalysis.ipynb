{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/glove-global-vectors-for-word-representation","execution_count":4,"outputs":[{"output_type":"stream","text":"glove.6B.100d.txt  glove.6B.200d.txt  glove.6B.50d.txt\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimdb_dir = '../input/keras-imdb/aclImdb_v1/aclImdb' # Data directory\ntrain_dir = os.path.join(imdb_dir, 'train') # Get the path of the train set\n\n# Setup empty lists to fill\nlabels = []\ntexts = []\n\n# First go through the negatives, then through the positives\nfor label_type in ['neg', 'pos']:\n    # Get the sub path\n    dir_name = os.path.join(train_dir, label_type)\n    print('loading ',label_type)\n    # Loop over all files in path\n    for fname in tqdm(os.listdir(dir_name)):\n        \n        # Only consider text files\n        if fname[-4:] == '.txt':\n            # Read the text file and put it in the list\n            f = open(os.path.join(dir_name, fname))\n            texts.append(f.read())\n            f.close()\n            # Attach the corresponding label\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)","execution_count":5,"outputs":[{"output_type":"stream","text":"loading  neg\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 12500/12500 [00:10<00:00, 1153.23it/s]\n  1%|          | 112/12500 [00:00<00:11, 1115.41it/s]","name":"stderr"},{"output_type":"stream","text":"loading  pos\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 12500/12500 [00:09<00:00, 1279.97it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"We should have 25,000 texts and labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(labels), len(texts)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(25000, 25000)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Half of the reviews are positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nnp.mean(labels)","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"0.5"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Let's look at a positive review:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Label',labels[24002])\nprint(texts[24002])","execution_count":8,"outputs":[{"output_type":"stream","text":"Label 1\nIt is a story of Siberian village people from the beginning of 20th century till the 60ties. It is about passion and feelings, about Russian soul, and very romantic. This movie IS NOT action packed, it flowes slowely. In second part one can find great songs - Russian romances. It is much more better than Doctor Zhivago. The director of this movie moved to America and made Runaway Train for example.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"And a negative review:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Label',labels[1])\nprint(texts[1])","execution_count":9,"outputs":[{"output_type":"stream","text":"Label 0\nWhat a terrible, TERRIBLE, film! One of the worst movies I have seen in my life. I usually love movies like this, the whole \"A guy meets an eccentric woman who he likes, but he happens to already be involved with someone, who not right for him....\". I expected something predictable and I didn't mind. The movies are always entertaining mixing the right amount of romance with comedy, but not this one! Every single joke falls flat and the \"romance\" makes me want to vomit. The title character is one of the most \"please kill me\" characters that I have ever witnessed on my television, the \"eccentric woman\" isn't very eccentric, more like quirky and annoying. The \"other someone\" is the most reasonable, mature person in this film but also happens to be just as annoying. This films flat out sucks, there's no way around it, don't waste your time.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Tokenizing text**\nComputers can not work with words directly. To them, a word is just a meaningless row of characters. To work with words, we need to turn words into so called 'Tokens'. A token is a number that represents that word. Each word gets assigned a token. Tokens are usually assigned by word frequency. The most frequent words like 'a' or 'the' get tokens like 1 or 2 while less often used words like 'profusely' get assigned very high numbers.\n\nWe can tokenize text directly with Keras. When we tokenize text, we usually choose a maximum number of words we want to consider, our vocabulary so to speak. This prevents us from assigning tokens to words that are hardly ever used, mostly because of typos or because they are not actual words or because they are just very uncommon. This prevents us from over fitting to texts that contain strange words or wired spelling errors. Words that are beyond that cutoff point get assigned the token 0, unknown."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nimport numpy as np\n\nmax_words = 10000 # We will only consider the 10K most used words in this dataset\n\ntokenizer = Tokenizer(num_words=max_words) # Setup\ntokenizer.fit_on_texts(texts) # Generate tokens by counting frequency\nsequences = tokenizer.texts_to_sequences(texts) # Turn text into sequence of numbers","execution_count":10,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Token for \"the\"',word_index['the'])\nprint('Token for \"Movie\"',word_index['movie'])\nprint('Token for \"generator\"',word_index['generator'])","execution_count":11,"outputs":[{"output_type":"stream","text":"Token for \"the\" 1\nToken for \"Movie\" 17\nToken for \"generator\" 20287\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the first 10 words of the sequence tokenized\nsequences[24002][:10]","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"[9, 6, 3, 62, 4, 2054, 81, 36, 1, 451]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nmaxlen = 100 # Make all sequences 100 words long\ndata = pad_sequences(sequences, maxlen=maxlen)\nprint(data.shape) # We have 25K, 100 word sequences now","execution_count":13,"outputs":[{"output_type":"stream","text":"(25000, 100)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.asarray(labels)\n\n# Shuffle data\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\ntraining_samples = 20000  # We will be training on 10K samples\nvalidation_samples = 5000  # We will be validating on 10000 samples\n\n# Split data\nx_train = data[:training_samples]\ny_train = labels[:training_samples]\nx_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":15,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 100, 50)           500000    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 5000)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                160032    \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 660,065\nTrainable params: 660,065\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_val, y_val))","execution_count":17,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","name":"stderr"},{"output_type":"stream","text":"Train on 20000 samples, validate on 5000 samples\nEpoch 1/10\n20000/20000 [==============================] - 5s 251us/step - loss: 0.4384 - acc: 0.7806 - val_loss: 0.3515 - val_acc: 0.8466\nEpoch 2/10\n20000/20000 [==============================] - 5s 228us/step - loss: 0.1172 - acc: 0.9616 - val_loss: 0.4452 - val_acc: 0.8278\nEpoch 3/10\n20000/20000 [==============================] - 5s 261us/step - loss: 0.0109 - acc: 0.9984 - val_loss: 0.5335 - val_acc: 0.8420\nEpoch 4/10\n20000/20000 [==============================] - 5s 255us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.5753 - val_acc: 0.8452\nEpoch 5/10\n20000/20000 [==============================] - 5s 229us/step - loss: 4.6307e-04 - acc: 1.0000 - val_loss: 0.6081 - val_acc: 0.8464\nEpoch 6/10\n20000/20000 [==============================] - 4s 221us/step - loss: 2.5724e-04 - acc: 1.0000 - val_loss: 0.6331 - val_acc: 0.8464\nEpoch 7/10\n20000/20000 [==============================] - 4s 221us/step - loss: 1.5602e-04 - acc: 1.0000 - val_loss: 0.6569 - val_acc: 0.8468\nEpoch 8/10\n20000/20000 [==============================] - 4s 223us/step - loss: 1.0012e-04 - acc: 1.0000 - val_loss: 0.6801 - val_acc: 0.8482\nEpoch 9/10\n20000/20000 [==============================] - 5s 229us/step - loss: 6.6070e-05 - acc: 1.0000 - val_loss: 0.7021 - val_acc: 0.8482\nEpoch 10/10\n20000/20000 [==============================] - 5s 233us/step - loss: 4.4428e-05 - acc: 1.0000 - val_loss: 0.7231 - val_acc: 0.8480\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Note that training your own embeddings is prone to over fitting. As you can see our model archives 100% accuracy on the training set but only 83% accuracy on the validation set. A clear sign of over fitting. In practice it is therefore quite rare to train new embeddings unless you have a massive dataset. Much more commonly, pre trained embeddings are used. A common pretrained embedding is GloVe, Global Vectors for Word Representation. It has been trained on billions of words from Wikipedia and the Gigaword 5 dataset, more than we could ever hope to train from our movie reviews. After downloading the GloVe embeddings from the GloVe website we can load them into our model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_dir = '../input/glove-global-vectors-for-word-representation' # This is the folder with the dataset\n\nprint('Loading word vectors')\nembeddings_index = {} # We create a dictionary of word -> embedding\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt')) # Open file\n\n# In the dataset, each line represents a new word embedding\n# The line starts with the word and the embedding values follow\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0] # The first value is the word, the rest are the values of the embedding\n    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n    embeddings_index[word] = embedding # Add embedding to our embedding dictionary\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":19,"outputs":[{"output_type":"stream","text":"1625it [00:00, 16247.03it/s]","name":"stderr"},{"output_type":"stream","text":"Loading word vectors\n","name":"stdout"},{"output_type":"stream","text":"400000it [00:24, 16457.78it/s]","name":"stderr"},{"output_type":"stream","text":"Found 400000 word vectors.\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a matrix of all embeddings\nall_embs = np.stack(embeddings_index.values())\nemb_mean = all_embs.mean() # Calculate mean\nemb_std = all_embs.std() # Calculate standard deviation\nemb_mean,emb_std","execution_count":20,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3242: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  if (await self.run_code(code, result,  async_=asy)):\n","name":"stderr"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"(0.004451992, 0.4081574)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 100 # We now use larger embeddings\n\nword_index = tokenizer.word_index\nnb_words = min(max_words, len(word_index)) # How many words are there actually\n\n# Create a random matrix with the same mean and std as the embeddings\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n\n# The vectors need to be in the same position as their index. \n# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n\n# Loop over all words in the word index\nfor word, i in word_index.items():\n    # If we are above the amount of words we want to use we do nothing\n    if i >= max_words: \n        continue\n    # Get the embedding vector for the word\n    embedding_vector = embeddings_index.get(word)\n    # If there is an embedding vector, put it in the embedding matrix\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights = [embedding_matrix], trainable = False))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":22,"outputs":[{"output_type":"stream","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 100, 100)          1000000   \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 10000)             0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 32)                320032    \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 1,320,065\nTrainable params: 320,065\nNon-trainable params: 1,000,000\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_val, y_val))","execution_count":24,"outputs":[{"output_type":"stream","text":"Train on 20000 samples, validate on 5000 samples\nEpoch 1/10\n20000/20000 [==============================] - 4s 181us/step - loss: 0.6231 - acc: 0.6395 - val_loss: 0.5632 - val_acc: 0.7114\nEpoch 2/10\n20000/20000 [==============================] - 3s 165us/step - loss: 0.5006 - acc: 0.7603 - val_loss: 0.5674 - val_acc: 0.7030\nEpoch 3/10\n20000/20000 [==============================] - 3s 165us/step - loss: 0.4348 - acc: 0.8000 - val_loss: 0.5716 - val_acc: 0.7188\nEpoch 4/10\n20000/20000 [==============================] - 3s 167us/step - loss: 0.3971 - acc: 0.8239 - val_loss: 0.6103 - val_acc: 0.7106\nEpoch 5/10\n20000/20000 [==============================] - 3s 168us/step - loss: 0.3709 - acc: 0.8346 - val_loss: 0.6023 - val_acc: 0.7174\nEpoch 6/10\n20000/20000 [==============================] - 3s 165us/step - loss: 0.3516 - acc: 0.8460 - val_loss: 0.6853 - val_acc: 0.6980\nEpoch 7/10\n20000/20000 [==============================] - 3s 167us/step - loss: 0.3269 - acc: 0.8573 - val_loss: 0.6560 - val_acc: 0.7104\nEpoch 8/10\n20000/20000 [==============================] - 3s 161us/step - loss: 0.3029 - acc: 0.8684 - val_loss: 0.7084 - val_acc: 0.7116\nEpoch 9/10\n20000/20000 [==============================] - 3s 161us/step - loss: 0.2854 - acc: 0.8765 - val_loss: 0.7753 - val_acc: 0.7074\nEpoch 10/10\n20000/20000 [==============================] - 3s 164us/step - loss: 0.2636 - acc: 0.8878 - val_loss: 0.7938 - val_acc: 0.7096\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Demo on a positive text\nmy_text = 'I love dogs. Dogs are the best. They are lovely, cuddly animals that only want the best for humans.'\n\nseq = tokenizer.texts_to_sequences([my_text])\nprint('raw seq:',seq)\nseq = pad_sequences(seq, maxlen=maxlen)\nprint('padded seq:',seq)\nprediction = model.predict(seq)\nprint('positivity:',prediction)","execution_count":25,"outputs":[{"output_type":"stream","text":"raw seq: [[10, 116, 2518, 2518, 23, 1, 115, 33, 23, 1331, 1383, 12, 61, 178, 1, 115, 15, 1707]]\npadded seq: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0   10  116\n  2518 2518   23    1  115   33   23 1331 1383   12   61  178    1  115\n    15 1707]]\npositivity: [[0.98324835]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Demo on a negative text\nmy_text = 'The bleak economic outlook will force many small businesses into bankruptcy.'\n\nseq = tokenizer.texts_to_sequences([my_text])\nprint('raw seq:',seq)\nseq = pad_sequences(seq, maxlen=maxlen)\nprint('padded seq:',seq)\nprediction = model.predict(seq)\nprint('positivity:',prediction)","execution_count":26,"outputs":[{"output_type":"stream","text":"raw seq: [[1, 3762, 7037, 77, 1145, 108, 389, 80]]\npadded seq: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    1 3762 7037   77 1145  108\n   389   80]]\npositivity: [[0.07994457]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}