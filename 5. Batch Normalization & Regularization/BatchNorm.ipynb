{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BatchNorm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dILEvJnCo8Q",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainX, trainy), (testX, testy) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZe621kNzX9a",
        "colab_type": "text"
      },
      "source": [
        "### reshape dataset to have a single channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77d77845-5332-459c-bea2-5665b248feb0"
      },
      "source": [
        "width, height, channels = trainX.shape[1], trainX.shape[2], 1\n",
        "print(width, height, channels)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28 28 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrnvfqKEx6Dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = trainX.reshape((trainX.shape[0], width, height, channels))\n",
        "testX = testX.reshape((testX.shape[0], width, height, channels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XB_4A8b3Quw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Convert 1-dimensional class arrays to 10-dimensional class matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SolM46jo3Q7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hold original y in variable to figure out miss values later on\n",
        "original_testy = testy\n",
        "\n",
        "trainy = np_utils.to_categorical(trainy, 10)\n",
        "testy = np_utils.to_categorical(testy, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzhGo2hNCRmR",
        "colab_type": "text"
      },
      "source": [
        "### report pixel means and standard deviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utbJFZOkzlJj",
        "colab_type": "code",
        "outputId": "12854445-235d-4dee-f5f8-e5c36eb39774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Statistics train=%.3f (%.3f), test=%.3f (%.3f)' % (trainX.mean(), trainX.std(), testX.mean(), testX.std()))"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics train=33.318 (78.567), test=33.791 (79.172)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM_jwjCPzndZ",
        "colab_type": "text"
      },
      "source": [
        "### create generator that centers pixel values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-CLOm8uzohs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbcffb05-3059-4408-fe6b-7d73c508fc91"
      },
      "source": [
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "datagen"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.preprocessing.image.ImageDataGenerator at 0x7f4fcead4a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_ItlO1PzqBj",
        "colab_type": "text"
      },
      "source": [
        "### calculate the mean on the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYXDdhhxzsRj",
        "colab_type": "code",
        "outputId": "22c083f1-f481-478d-95c8-1bcb50f933dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "datagen.fit(trainX)\n",
        "print('Data Generator mean=%.3f, std=%.3f' % (datagen.mean, datagen.std))"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Generator mean=33.318, std=78.567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUQTxIQ3zvN5",
        "colab_type": "text"
      },
      "source": [
        "### demonstrate effect on a single batch of samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLCsmXOAzxTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45dd3c01-a191-4d6d-9047-5f527683e998"
      },
      "source": [
        "iterator = datagen.flow(trainX, trainy, batch_size=64)\n",
        "iterator"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.image.numpy_array_iterator.NumpyArrayIterator at 0x7f4fcea77eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "643dGJS0zyyy",
        "colab_type": "text"
      },
      "source": [
        "### get a batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUnm92eTz0E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchX, batchy = iterator.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUxb_Vzhz2Rp",
        "colab_type": "text"
      },
      "source": [
        "### pixel stats in the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCNTt0Jzz1cy",
        "colab_type": "code",
        "outputId": "83cdbacb-4432-43d6-a3f1-bcb6a5109d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(batchX.shape, batchX.mean(), batchX.std())"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 28, 28, 1) -0.0026255564 0.9992786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25L8irx7z67U",
        "colab_type": "text"
      },
      "source": [
        "### demonstrate effect on entire training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrV86QW1z8_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69954936-5821-42ab-c01f-ddf26b13d98f"
      },
      "source": [
        "iterator = datagen.flow(trainX, trainy, batch_size=len(trainX), shuffle=False)\n",
        "iterator"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.image.numpy_array_iterator.NumpyArrayIterator at 0x7f4fcea31748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-nrLWXQE91y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model = Sequential()\n",
        "# Extract edges and gradients features\n",
        "model.add(Convolution2D(16, (3, 3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu', input_shape=(28,28,1))) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# Trying to extract more features by increasing channels \n",
        "model.add(Convolution2D(16, (3, 3),  kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),activation='relu')) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# Trying to extract more features by increasing channels \n",
        "model.add(Convolution2D(16, (3, 3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu')) #22\n",
        "\n",
        "# reducing the size of parameters\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #11\n",
        "\n",
        "# Since we have done  MP above we should try using 1x1 and fetch co dependend features.\n",
        "model.add(Convolution2D(16, (1, 1), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu')) #11\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# Trying to Increase the channels to fetch parts of object\n",
        "model.add(Convolution2D(16, (3, 3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu')) #9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu')) #7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, (3, 3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu')) #5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, (3,3))) #3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, (3,3))) \n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "1bf14f1b-d46b-4f84-a2a6-17a18cfcda04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_118 (Conv2D)          (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_92 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_119 (Conv2D)          (None, 24, 24, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_93 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_93 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_120 (Conv2D)          (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_121 (Conv2D)          (None, 11, 11, 16)        272       \n",
            "_________________________________________________________________\n",
            "batch_normalization_94 (Batc (None, 11, 11, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_94 (Dropout)         (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_122 (Conv2D)          (None, 9, 9, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_95 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_95 (Dropout)         (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_123 (Conv2D)          (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_96 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_96 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_124 (Conv2D)          (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_97 (Batc (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_97 (Dropout)         (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_125 (Conv2D)          (None, 3, 3, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_98 (Batc (None, 3, 3, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_98 (Dropout)         (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_126 (Conv2D)          (None, 1, 1, 10)          1450      \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,250\n",
            "Trainable params: 16,026\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHiUfCicFon6",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "17fef4cb-90c0-493f-bbac-c296f5b0f752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4131
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "\n",
        "filepath=\"model_accuracy.best.hdf5\"\n",
        "\n",
        "reduce_LR = LearningRateScheduler(scheduler, verbose=1)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "#callbacks_list = [reduce_LR, checkpoint]\n",
        "\n",
        "model.fit(trainX, trainy, batch_size=128, epochs=40, verbose=1, validation_data=(testX, testy), callbacks=[reduce_LR, checkpoint])"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.5894 - acc: 0.9289 - val_loss: 0.5941 - val_acc: 0.8199\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.81990, saving model to model_accuracy.best.hdf5\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1690 - acc: 0.9733 - val_loss: 0.1487 - val_acc: 0.9756\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.81990 to 0.97560, saving model to model_accuracy.best.hdf5\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1350 - acc: 0.9777 - val_loss: 0.1150 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97560 to 0.98250, saving model to model_accuracy.best.hdf5\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1198 - acc: 0.9790 - val_loss: 0.0993 - val_acc: 0.9842\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98250 to 0.98420, saving model to model_accuracy.best.hdf5\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.1055 - acc: 0.9814 - val_loss: 0.0983 - val_acc: 0.9849\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98420 to 0.98490, saving model to model_accuracy.best.hdf5\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0996 - acc: 0.9825 - val_loss: 0.1027 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.98490\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 8s 136us/step - loss: 0.0923 - acc: 0.9839 - val_loss: 0.0755 - val_acc: 0.9883\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98490 to 0.98830, saving model to model_accuracy.best.hdf5\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 9s 148us/step - loss: 0.0878 - acc: 0.9844 - val_loss: 0.0785 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98830\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 8s 140us/step - loss: 0.0848 - acc: 0.9842 - val_loss: 0.0707 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.98830 to 0.98950, saving model to model_accuracy.best.hdf5\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0793 - acc: 0.9857 - val_loss: 0.0761 - val_acc: 0.9857\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98950\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0741 - acc: 0.9871 - val_loss: 0.0643 - val_acc: 0.9891\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.98950\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0683 - acc: 0.9875 - val_loss: 0.0654 - val_acc: 0.9885\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.98950\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0695 - acc: 0.9866 - val_loss: 0.0564 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.98950 to 0.99110, saving model to model_accuracy.best.hdf5\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0664 - acc: 0.9875 - val_loss: 0.0596 - val_acc: 0.9891\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99110\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0644 - acc: 0.9883 - val_loss: 0.0500 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.99110 to 0.99180, saving model to model_accuracy.best.hdf5\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0624 - acc: 0.9882 - val_loss: 0.0499 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.99180 to 0.99200, saving model to model_accuracy.best.hdf5\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0587 - acc: 0.9892 - val_loss: 0.0490 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99200\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0581 - acc: 0.9888 - val_loss: 0.0506 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99200\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0581 - acc: 0.9887 - val_loss: 0.0506 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99200\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0535 - acc: 0.9901 - val_loss: 0.0450 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.99200 to 0.99240, saving model to model_accuracy.best.hdf5\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0541 - acc: 0.9900 - val_loss: 0.0459 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.99240 to 0.99260, saving model to model_accuracy.best.hdf5\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0509 - acc: 0.9904 - val_loss: 0.0450 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99260\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0494 - acc: 0.9904 - val_loss: 0.0443 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99260\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0511 - acc: 0.9904 - val_loss: 0.0424 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.99260 to 0.99270, saving model to model_accuracy.best.hdf5\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0498 - acc: 0.9902 - val_loss: 0.0409 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99270\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0472 - acc: 0.9911 - val_loss: 0.0422 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99270\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0474 - acc: 0.9911 - val_loss: 0.0423 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.99270\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0476 - acc: 0.9903 - val_loss: 0.0438 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99270\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0450 - acc: 0.9913 - val_loss: 0.0397 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.99270 to 0.99280, saving model to model_accuracy.best.hdf5\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0436 - acc: 0.9912 - val_loss: 0.0407 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99280\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0444 - acc: 0.9910 - val_loss: 0.0383 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99280\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0420 - acc: 0.9914 - val_loss: 0.0526 - val_acc: 0.9888\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99280\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0425 - acc: 0.9914 - val_loss: 0.0430 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99280\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0410 - acc: 0.9919 - val_loss: 0.0362 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.99280 to 0.99380, saving model to model_accuracy.best.hdf5\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0415 - acc: 0.9914 - val_loss: 0.0390 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99380\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0401 - acc: 0.9923 - val_loss: 0.0369 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99380\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "60000/60000 [==============================] - 8s 135us/step - loss: 0.0397 - acc: 0.9922 - val_loss: 0.0400 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99380\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0380 - acc: 0.9927 - val_loss: 0.0352 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99380\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0391 - acc: 0.9926 - val_loss: 0.0407 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99380\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0385 - acc: 0.9920 - val_loss: 0.0356 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99380\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4fd0760a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSUVq9vxM-wh",
        "colab_type": "text"
      },
      "source": [
        "### Wrongly predicted data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltCjuHLKRxbn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "ab78bfeb-0c1b-4383-f808-318b2553649a"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import array_to_img\n",
        "from keras.preprocessing.image import save_img\n",
        "\n",
        "prediction = np.round(model.predict(testX))\n",
        "wrong_pred = np.flatnonzero(testy != prediction)\n",
        "print(wrong_pred)\n",
        "#testy[9999]\n"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-296-f51580be37e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mtesty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0msubset_of_wrongly_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msubset_of_wrongly_predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-296-f51580be37e7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mtesty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0msubset_of_wrongly_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msubset_of_wrongly_predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2IpGuyKa1o7",
        "colab_type": "text"
      },
      "source": [
        "### Save Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMhpxRxDa10J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for index in wrong_pred:\n",
        "  if(count <25):\n",
        "    #print(testX[index])\n",
        "    #img_pil = array_to_img(testX[index])\n",
        "    fileName = 'Wrong_Predicted_Image_' + str(index) + '.jpg'\n",
        "    save_img(fileName, testX[index])\n",
        "    #print(type(img_pil))\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(testX, testy, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29e7dfa7-6738-4fbf-a073-b982aadb487f"
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.03563580178171396, 0.9931]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(testX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "72e77fd4-1940-4477-c624-d6bccb3f9dee"
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(testy[:9])"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.46519173e-13 5.60730529e-09 1.52785702e-07 1.64927783e-07\n",
            "  1.20866884e-11 1.30508417e-11 1.15096182e-15 9.99999642e-01\n",
            "  1.52352974e-11 9.83923254e-09]\n",
            " [1.39181418e-06 1.20113327e-06 9.99996185e-01 2.29654837e-08\n",
            "  1.21730936e-09 1.67907008e-11 1.17422246e-06 6.68686551e-10\n",
            "  9.12800147e-09 4.47066759e-11]\n",
            " [1.72814918e-08 9.99985456e-01 5.50655308e-08 2.20120988e-09\n",
            "  6.83030339e-06 2.69551936e-08 1.72214655e-07 7.07868412e-06\n",
            "  2.29320580e-08 4.60192666e-07]\n",
            " [9.99952793e-01 2.30607777e-09 4.71511339e-06 6.71190747e-08\n",
            "  3.04885859e-08 1.55618110e-07 3.62793180e-05 7.83545318e-08\n",
            "  5.43776059e-06 4.32432500e-07]\n",
            " [1.62186944e-10 6.50429710e-10 6.15694440e-11 9.30903900e-13\n",
            "  9.99997377e-01 3.11955635e-11 1.96403130e-10 2.60728528e-09\n",
            "  2.62515315e-10 2.61970149e-06]\n",
            " [3.20917359e-09 9.99948978e-01 5.07923081e-09 1.46807538e-10\n",
            "  1.29080490e-05 5.06528020e-09 6.68927385e-08 3.70100024e-05\n",
            "  2.62352096e-09 1.08256324e-06]\n",
            " [1.75892755e-11 1.32267280e-06 6.39218589e-09 4.49945275e-10\n",
            "  9.99436796e-01 4.92862853e-07 9.85948961e-11 2.47242078e-05\n",
            "  3.05934227e-05 5.05974283e-04]\n",
            " [1.93406819e-07 9.63035651e-10 4.08352889e-08 6.43389797e-07\n",
            "  2.56568655e-05 2.76721408e-07 4.06254141e-10 3.58311780e-09\n",
            "  9.03591626e-06 9.99964118e-01]\n",
            " [1.45260674e-06 1.01635225e-08 6.39490239e-08 1.60438518e-07\n",
            "  6.85128354e-09 9.08829272e-01 9.10013616e-02 3.89643873e-09\n",
            "  1.58811730e-04 8.75945443e-06]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tvptcn8dxvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}